
> *Like learning a language by predicting the next word in a sentence - you create your own exercises from raw text.*

**Parent Note:** [[1. Types of Machine Learning]]

---

## What is Self-Supervised Learning?

Self-supervised learning **generates labeled data from unlabeled data** automatically. The model creates its own supervision signal by designing tasks where the labels come directly from the data itself.

**Analogy:** *Like doing a jigsaw puzzle - you don't need someone to tell you which pieces go together; the pieces themselves show you through their shapes and patterns.*

### Key Concept

**We transform unlabeled data into a supervised learning problem by:**
- Hiding part of the data
- Making the model predict the hidden part
- The hidden part becomes the label

---

## How Self-Supervised Learning Works

```mermaid
graph TD
    START[Unlabeled Data] --> TRANSFORM[Create Pretext Task<br/>Hide/Modify data]
    TRANSFORM --> LABEL[Generate Labels<br/>from the data itself]
    LABEL --> TRAIN[Train Model<br/>to predict labels]
    TRAIN --> LEARN[Model Learns<br/>Useful Representations]
    LEARN --> APPLY[Apply to<br/>Downstream Tasks]
    
    style START fill:#2196F3,color:#fff
    style LABEL fill:#FF9800,color:#fff
    style LEARN fill:#4CAF50,color:#fff
    style APPLY fill:#9C27B0,color:#fff
```

### The Process

1. Take unlabeled data
2. Design a pretext task that creates labels automatically
3. Train model on this self-created task
4. Model learns useful representations
5. Fine-tune for actual task

---

## Example 1: Predicting Next Word

**Analogy:** *Like completing sentences in your head while reading - you naturally predict what comes next.*

### The Task

**Given:** "The cat sat on the ___"

**Unlabeled Data:**
```
The cat sat on the mat.
```

**Self-Created Labels:**

| Input (Context) | Label (Next Word) |
|----------------|-------------------|
| "The" | "cat" |
| "The cat" | "sat" |
| "The cat sat" | "on" |
| "The cat sat on" | "the" |
| "The cat sat on the" | "mat" |

**Process:**
```mermaid
graph LR
    TEXT[Unlabeled Text:<br/>The cat sat on the mat] --> SPLIT[Split into<br/>Context + Target]
    SPLIT --> TASK[Predict Next Word]
    TASK --> LABELS[Auto-Generated Labels]
    LABELS --> TRAIN[Train Language Model]
    
    style TEXT fill:#2196F3,color:#fff
    style LABELS fill:#FF9800,color:#fff
    style TRAIN fill:#4CAF50,color:#fff
```

**Result:** Model learns language patterns, grammar, and context without any manual labeling.

---

## Example 2: Image Rotation Prediction

**Analogy:** *Like learning what "right-side up" looks like by seeing objects in different orientations.*

### The Task

Take an image and rotate it by 0Â°, 90Â°, 180Â°, or 270Â°. The model must predict the rotation angle.

**Original Unlabeled Image:**
```
ðŸ  (House image)
```

**Self-Created Dataset:**

| Rotated Image | Auto-Generated Label |
|---------------|---------------------|
| ðŸ  (0Â°) | 0 |
| ðŸšï¸ (90Â°) | 90 |
| ðŸ  (180Â°) | 180 |
| ðŸšï¸ (270Â°) | 270 |

```mermaid
graph TD
    IMG[Unlabeled Image] --> ROT[Apply Rotations<br/>0Â°, 90Â°, 180Â°, 270Â°]
    ROT --> AUTO[Auto-Generated Labels:<br/>Rotation Angle]
    AUTO --> TRAIN[Train Model to<br/>Predict Rotation]
    TRAIN --> LEARN[Model Learns<br/>Object Structure & Orientation]
    
    style IMG fill:#2196F3,color:#fff
    style AUTO fill:#FF9800,color:#fff
    style LEARN fill:#4CAF50,color:#fff
```

**What the Model Learns:**
- Object shapes and boundaries
- Spatial relationships
- Visual features
- Orientation awareness

---

## Example 3: Masked Image Modeling

**Analogy:** *Like looking at a photo with some parts covered and guessing what's hidden.*

### The Task

Hide parts of an image and make the model predict what's missing.

**Process:**

```mermaid
graph LR
    ORIGINAL[Original Image] --> MASK[Mask Random Patches]
    MASK --> VISIBLE[Visible Patches:<br/>Input]
    MASK --> HIDDEN[Hidden Patches:<br/>Labels]
    VISIBLE --> PREDICT[Model Predicts<br/>Hidden Patches]
    HIDDEN --> COMPARE[Compare & Learn]
    
    style ORIGINAL fill:#2196F3,color:#fff
    style HIDDEN fill:#FF9800,color:#fff
    style COMPARE fill:#4CAF50,color:#fff
```

**Example:**
- Original: Full image of a cat on a sofa
- Masked: 25% of image patches are hidden
- Task: Predict the pixel values of hidden patches
- Label: The actual pixel values (from original image)

---

## Example 4: Contrastive Learning

**Analogy:** *Like learning to recognize your friend by seeing many photos of them in different settings and learning what stays the same.*

### The Task

Create multiple views of the same image and teach the model that they represent the same thing.

```mermaid
graph TD
    IMG[Original Image:<br/>Cat Photo] --> AUG1[Augmentation 1:<br/>Crop + Flip]
    IMG --> AUG2[Augmentation 2:<br/>Color Change + Rotate]
    AUG1 --> SIMILAR[These Should Be<br/>SIMILAR]
    AUG2 --> SIMILAR
    OTHER[Other Images] --> DISSIMILAR[These Should Be<br/>DIFFERENT]
    
    style IMG fill:#2196F3,color:#fff
    style SIMILAR fill:#4CAF50,color:#fff
    style DISSIMILAR fill:#F44336,color:#fff
```

**Self-Created Labels:**
- Augmented versions of same image: Label = "Similar"
- Different images: Label = "Dissimilar"

---

## Common Self-Supervised Tasks

```mermaid
graph TB
    SSL[Self-Supervised<br/>Learning Tasks] --> NLP[Natural Language<br/>Processing]
    SSL --> CV[Computer Vision]
    SSL --> AUDIO[Audio/Speech]
    
    NLP --> NLP1[Next Word Prediction]
    NLP --> NLP2[Masked Word Prediction]
    NLP --> NLP3[Sentence Order Prediction]
    
    CV --> CV1[Image Rotation]
    CV --> CV2[Masked Image Modeling]
    CV --> CV3[Contrastive Learning]
    CV --> CV4[Jigsaw Puzzle]
    
    AUDIO --> AUDIO1[Masked Audio Prediction]
    AUDIO --> AUDIO2[Audio-Visual Matching]
    
    style SSL fill:#9C27B0,color:#fff
    style NLP fill:#4CAF50,color:#fff
    style CV fill:#2196F3,color:#fff
    style AUDIO fill:#FF9800,color:#fff
```

### Task Categories

| Domain | Task | Input | Auto-Generated Label |
|--------|------|-------|---------------------|
| **Text** | Next word prediction | "The cat sat on" | "the" |
| **Text** | Masked word | "The [MASK] sat" | "cat" |
| **Image** | Rotation | Rotated image | Rotation angle (0Â°, 90Â°, 180Â°, 270Â°) |
| **Image** | Jigsaw | Shuffled patches | Correct patch order |
| **Image** | Colorization | Grayscale image | Original colors |
| **Video** | Frame order | Shuffled frames | Correct sequence |
| **Audio** | Masked audio | Audio with gaps | Missing audio segment |

---

## Self-Supervised vs Semi-Supervised

**Key Difference:** How labels are obtained

```mermaid
graph TB
    DATA[Unlabeled Data] --> SEMI[Semi-Supervised]
    DATA --> SELF[Self-Supervised]
    
    SEMI --> SEMI_LABEL[Some human-provided labels<br/>+ model predictions<br/>on unlabeled data]
    SELF --> SELF_LABEL[Automatically generated labels<br/>from data structure<br/>No human labels needed]
    
    style DATA fill:#2196F3,color:#fff
    style SEMI fill:#FF9800,color:#fff
    style SELF fill:#00BCD4,color:#fff
```

| Aspect | Semi-Supervised | Self-Supervised |
|--------|----------------|-----------------|
| **Human Labels** | Few needed initially | None needed |
| **Label Source** | Human + pseudo-labels | Data structure itself |
| **Example** | 100 labeled + 10K unlabeled images | Predict rotation of images |
| **Analogy** | Learn from few examples + practice | Create your own practice problems |
| **Cost** | Medium (some labeling) | Very low (no labeling) |

---

## Real-World Applications

### 1. Language Models (GPT, BERT)

**Task:** Predict next word / masked word
**Data:** Massive text from internet (unlabeled)
**Result:** Models that understand language

### 2. Computer Vision (SimCLR, MoCo)

**Task:** Contrastive learning on images
**Data:** Millions of unlabeled images
**Result:** Models that understand visual features

### 3. Speech Recognition

**Task:** Predict masked audio segments
**Data:** Unlabeled audio recordings
**Result:** Models that understand speech patterns

---

## Why Self-Supervised Learning is Powerful

```mermaid
graph LR
    POWER[Why Powerful?] --> SCALE[Leverage Massive<br/>Unlabeled Data]
    POWER --> COST[No Labeling<br/>Cost]
    POWER --> GENERAL[Learn General<br/>Representations]
    
    SCALE --> EX1[Train on billions<br/>of web images]
    COST --> EX2[Use all internet text<br/>without manual tagging]
    GENERAL --> EX3[Transfer learning<br/>to many tasks]
    
    style POWER fill:#9C27B0,color:#fff
    style SCALE fill:#4CAF50,color:#fff
    style COST fill:#4CAF50,color:#fff
    style GENERAL fill:#4CAF50,color:#fff
```

### Key Advantages

1. **Scalability:** Can use unlimited unlabeled data
2. **Cost-Effective:** No expensive human labeling
3. **Generalization:** Learns broadly useful features
4. **Transfer Learning:** Pre-trained models work on many tasks

---

## The Big Picture

```mermaid
graph TD
    UNL[Massive Unlabeled Data<br/>Internet text, images, videos] --> SELF[Self-Supervised Learning<br/>Create pretext tasks]
    SELF --> PRETRAIN[Pre-trained Model<br/>Learned representations]
    PRETRAIN --> FINE[Fine-tune for<br/>Specific Tasks]
    
    FINE --> TASK1[Image Classification]
    FINE --> TASK2[Text Generation]
    FINE --> TASK3[Object Detection]
    
    style UNL fill:#2196F3,color:#fff
    style SELF fill:#00BCD4,color:#fff
    style PRETRAIN fill:#4CAF50,color:#fff
```

---

## Quick Summary

**Self-supervised learning:**
- Generates labels **automatically from unlabeled data**
- Creates **pretext tasks** (rotation, masking, prediction)
- No human labeling required
- Powers modern AI (GPT, BERT, image models)
- Enables learning from massive datasets

**Remember:** The data provides its own supervision through clever task design.

---

## Related Notes

- [[0. Machine Learning Terms]]
- [[1. Types of Machine Learning]] - Overview of all ML types
- [[2. Supervised Machine Learning]]
- [[3. Unsupervised Machine Learning]]
- [[4. Semi-Supervised Machine Learning]]

---

#self-supervised-learning #machine-learning #pretext-tasks #representation-learning #ai

