---
tags: [topic, status/placeholder]
lecture: [15]
created: 2026-02-13
---
# Polynomial Regression

## Overview

Polynomial regression extends linear regression to model non-linear relationships between variables by introducing polynomial terms. While the relationship between predictors and response becomes non-linear, the model remains linear in its parameters.

## Motivation

Simple linear regression assumes a straight-line relationship:

- **Simple Linear Regression**: `y = β₀ + β₁x`
- **Multiple Linear Regression**: `y = β₀ + β₁x₁ + β₂x₂`

However, many real-world relationships are curved, not linear. A straight line may not be the best fit for complex data patterns.

## Types of Polynomial Regression

### 1. Simple Polynomial Regression

Models non-linear relationships using powers of a single independent variable.

**Degree = 2** (Quadratic):

```
y = β₀ + β₁x + β₂x²
```

**Degree = 3** (Cubic):

```
y = β₀ + β₁x + β₂x² + β₃x³
```

**General form** (Degree = d):

```
y = β₀ + β₁x + β₂x² + β₃x³ + ... + βₐxᵈ
```

#### Feature Transformation

When degree = 2, a single feature X is transformed into three features:

- x⁰ = 1 (intercept term)
- x¹ = x
- x² = x²

This transforms 1 independent column into 3 independent columns.

### 2. Multiple Polynomial Regression

Extends polynomial regression to multiple independent variables, creating polynomial terms for each variable.

**Example with 2 variables and degree = 3**:

```
y = β₀ + β₁x₁ + β₂x₁² + β₃x₁³ + β₄x₂ + β₅x₂² + β₆x₂³
```

You can also include interaction terms:

```
y = β₀ + β₁x₁ + β₂x₂ + β₃x₁² + β₄x₂² + β₅x₁x₂ + ...
```

## Choosing the Optimal Degree

The degree is a **hyperparameter** that controls model complexity. Choosing the right degree is critical:

### Three Scenarios

1. **Degree too low (e.g., degree = 2)**
    
    - Model is **underfit**
    - Cannot capture the true underlying pattern
    - High bias, low variance
    - Poor performance on both training and test data
2. **Degree optimal (e.g., degree = 5)**
    
    - Model captures the true relationship
    - Balanced bias-variance tradeoff
    - Good generalization to new data
3. **Degree too high (e.g., degree = 20)**
    
    - Model is **overfit**
    - Fits training data perfectly, including noise
    - Low bias, high variance
    - Poor performance on test data

### Selection Methods

- **Cross-validation**: Test different degrees and select the one with best validation performance
- **Regularization**: Use Ridge or Lasso regression to prevent overfitting with higher degrees
- **Information criteria**: AIC, BIC for model comparison
- **Learning curves**: Plot training/validation error vs. degree

## Why Polynomial Regression is Still "Linear"

**Common Interview Question**: Why is polynomial regression called linear regression?

**Answer**: Polynomial regression is linear **with respect to the parameters (β)**, not the features (x).

The model is linear because:

- The output y is a **linear combination** of the parameters β₀, β₁, β₂, etc.
- Each parameter is multiplied by some function of x and then summed
- We're not taking powers or products of the β coefficients

```
y = β₀ + β₁x + β₂x²
```

- This is **linear in β** (we add/multiply β terms linearly)
- This is **non-linear in x** (x appears in powers)

The feature values (x, x², x³) are treated as independent variables once computed. The model parameters determine performance.

## Training Polynomial Regression

### Parameter Estimation

Both standard linear regression methods work:

1. **Ordinary Least Squares (OLS)**
    
    - Closed-form solution: `β = (XᵀX)⁻¹Xᵀy`
    - Minimizes sum of squared residuals
    - Fast but can be computationally expensive for large datasets
2. **Gradient Descent**
    
    - Iterative optimization
    - Minimizes loss function step by step
    - Better for large datasets or when XᵀX is not invertible

### Loss Function

Same as linear regression - Mean Squared Error (MSE):

```
MSE = (1/n) Σ(yᵢ - ŷᵢ)²
```

### Model Performance

To alter model performance, you can only adjust:

- **Model parameters (β)**: Changed during training via OLS or gradient descent
- **Hyperparameters**: Degree, regularization strength (if using Ridge/Lasso)

The feature values (x) are fixed from the data. Performance depends entirely on how the parameters are learned.

## Overfitting Concerns

### The Danger of High Degrees

- Higher degree polynomials can fit training data almost perfectly
- They may capture noise instead of signal
- Result: Poor generalization to new data

### Prevention Strategies

1. **Regularization** (Ridge/Lasso)
    
    - Add penalty term to loss function
    - Prevents coefficients from growing too large
    - Particularly important for high-degree polynomials
2. **Cross-validation**
    
    - Evaluate performance on held-out data
    - Select degree that minimizes validation error
3. **Early stopping**
    
    - When using gradient descent, stop before overfitting occurs
4. **Feature selection**
    
    - Remove unnecessary polynomial terms
    - Use domain knowledge to guide feature engineering

## Practical Considerations

### Advantages

- Captures non-linear relationships
- Retains interpretability (compared to complex ML models)
- Uses familiar linear regression machinery
- Flexible - can model various curve shapes

### Disadvantages

- Sensitive to outliers (especially at higher degrees)
- Extrapolation can be unreliable (curves can diverge wildly outside data range)
- Feature scaling becomes critical (x² and x³ can have vastly different scales)
- Risk of overfitting with high degrees

### Best Practices

1. **Always scale features** before creating polynomial terms
2. **Start with low degrees** (2-3) and increase if needed
3. **Use cross-validation** to select optimal degree
4. **Apply regularization** for higher-degree polynomials
5. **Visualize** the fitted curve to check for sensible behavior

## Related Topics

- [[01. Multiple Linear Regression]] - Foundation for understanding polynomial regression
- [[09. Model Selection/03. Regularization Techniques]] - Ridge and Lasso for preventing overfitting

## Summary

Polynomial regression extends linear regression to model curved relationships by adding polynomial features. Despite the non-linear relationship with input features, it remains linear in parameters, allowing us to use standard linear regression techniques. The key challenge is selecting the right degree to balance model complexity and generalization.