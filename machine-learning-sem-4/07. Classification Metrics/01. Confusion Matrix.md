---
tags: [topic]
lecture: [18]
created: 2026-02-13
---

# Confusion Matrix

## Overview

A **Confusion Matrix** is a fundamental tool for evaluating classification model performance. It provides a complete picture of how your model classifies each sample by comparing predicted labels against actual (true) labels. Unlike simple accuracy, which can be misleading for imbalanced datasets, the confusion matrix reveals the nuances of classification performance.

For binary classification (two classes), the matrix is a 2×2 table with four key values. Understanding what each represents is essential for diagnosing model behavior and selecting appropriate metrics.

## Binary Classification Confusion Matrix

```
                        Predicted
                 ┌─────────────┬─────────────┐
                 │   Positive  │   Negative  │
       ┌─────────┼─────────────┼─────────────┤
Actual │ Positive│      TP     │      FN     │
       ├─────────┼─────────────┼─────────────┤
       │ Negative│      FP     │      TN     │
       └─────────┴─────────────┴─────────────┘
```

### The Four Outcomes

| Term | Full Name | Meaning | Example (Disease Test) |
|------|-----------|---------|----------------------|
| **TP** | True Positive | Correctly predicted positive | Test says "sick", patient is sick |
| **TN** | True Negative | Correctly predicted negative | Test says "healthy", patient is healthy |
| **FP** | False Positive | Incorrectly predicted positive | Test says "sick", patient is healthy |
| **FN** | False Negative | Incorrectly predicted negative | Test says "healthy", patient is sick |

### Important Analogies

**False Positive = Type I Error**
- "Crying wolf when there's no wolf"
- Example: Spam filter marks legitimate email as spam

**False Negative = Type II Error**
- "Missing a criminal"
- Example: Medical test fails to detect a disease

## Visual Representation

```
                    Predicted
              ┌───────────────────┐
              │   YES    │   NO   │
    ┌─────────┼──────────┼────────┤
 YES│         │    ✓     │   ✗    │
    │         │   (TP)   │  (FN)  │
    ├─────────┼──────────┼────────┤
 NO │         │    ✗     │   ✓    │
    │         │   (FP)   │  (TN)  │
    └─────────┴──────────────────┘
           Correct    Incorrect
```

## Cancer Detection Example

Let's say we have a cancer detection test:

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Actual Positive (Cancer) | 100 | |
| Actual Negative (Healthy) | 900 | |
| Predicted Positive | 120 | |
| Predicted Negative | 880 | |

The confusion matrix:

```
                    Predicted
              ┌─────────────────────┐
              │  Cancer  │  Healthy │
    ┌─────────┼──────────┼──────────┤
Cancer│       │    80    │    20    │
      │       │   (TP)   │   (FN)   │
    ├─────────┼──────────┼──────────┤
Healthy│     │    40    │   860    │
      │       │   (FP)   │   (TN)   │
    └─────────┴─────────────────────┘
```

From this we can calculate:
- **Accuracy**: (80 + 860) / 1000 = 94%
- **Precision**: 80 / (80 + 40) = 67%
- **Recall**: 80 / (80 + 20) = 80%

## Python Implementation

```python
from sklearn.metrics import confusion_matrix
import numpy as np

# Actual and predicted labels
y_actual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_predicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# Compute confusion matrix
cm = confusion_matrix(y_actual)
print(cm, y_predicted)

# Extract values
TN, FP, FN, TP = cm.ravel()
print(f"TN: {TN}, FP: {FP}, FN: {FN}, TP: {TP}")

# Calculate metrics
accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)
recall = TP / (TP + FN)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
```

## Multi-Class Confusion Matrix

For more than 2 classes, the matrix expands:

```
                 Predicted
        ┌─────────────────────────┐
        │  Cat  │  Dog  │  Bird  │
    ┌───┼────────┼────────┼────────┤
 Cat│   │   45   │   3    │   2    │
    ├───┼────────┼────────┼────────┤
 Dog│   │   5    │   50   │   5    │
    ├───┼────────┼────────┼────────┤
Bird│   │   1    │   4    │   40   │
    └───┴────────┴────────┴────────┘
```

The diagonal shows correct predictions; off-diagonal shows errors.

## When to Use Confusion Matrix

### Good For:
- Understanding what types of errors your model makes
- Imbalanced datasets (where accuracy is misleading)
- Multi-class problems
- Deciding between models

### Not Good For:
- Quick summary (use accuracy, precision, recall)
- Ranking models (use F1, AUC)

## Common Misconceptions

**Misconception**: "High accuracy means a good model"

**Reality**: With 99% negative samples, a model that always predicts "negative" gets 99% accuracy but is useless!

```
Dataset: 9900 Negative, 100 Positive
Model: Always predict "Negative"

Confusion Matrix:
              Predicted
              Neg    Pos
Actual Neg  [9900,   0]
Actual Pos  [100,    0]

Accuracy = 9900/10000 = 99%
But it never detects positives!
```

## Interview Questions

**Q: What's the difference between Type I and Type II errors?**
Type I Error (False Positive): Rejecting a true null hypothesis. Think "crying wolf when there's no wolf." Type II Error (False Negative): Failing to reject a false null hypothesis. Think "missing a criminal."

**Q: In medical diagnosis, which error is more serious?**
Generally, False Negative (missing a disease) is more serious—you want to err on the side of caution. But context matters: for some applications (like spam filtering), False Positives might be worse (missing an important email).

**Q: Can a confusion matrix be used for multi-class problems?**
Yes! It generalizes to an N×N matrix where N is the number of classes. The diagonal contains correct predictions; off-diagonal contains errors.

## Summary

A confusion matrix breaks down classification predictions into four categories:
- **TP (True Positive)**: Correctly predicted positive
- **TN (True Negative)**: Correctly predicted negative  
- **FP (False Positive)**: Wrongly predicted positive (Type I)
- **FN (False Negative)**: Wrongly predicted negative (Type II)

This granular view reveals model behavior that accuracy alone cannot show.

## Related Notes

- [[02. Precision and Recall]]
- [[03. F1 Score]]
- [[04. Loss vs Cost Function]]

## Resources

- Lectures: 18-20
