---
tags: [topic]
lecture: [18]
created: 2026-02-13
---

# F1 Score

## Overview

When you need a single metric that balances precision and recall, the **F1 Score** is the go-to solution. It's the **harmonic mean** of precision and recall, providing a balanced measure that penalizes extreme imbalance between the two. While precision and recall can be optimized at the expense of each other, F1 score forces you to consider both.

F1 is particularly valuable for imbalanced datasets where accuracy can be misleading.

## Mathematical Definition

### Formula

$$F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

Or expanded:

$$F_1 = \frac{2 \times TP}{2 \times TP + FP + FN}$$

### Why Harmonic Mean?

The harmonic mean penalizes extreme values. If either precision or recall is very low, F1 will also be low:

```
Arithmetic Mean: (0.9 + 0.1) / 2 = 0.50
Geometric Mean: sqrt(0.9 × 0.1) = 0.30
Harmonic Mean: 2 × 0.9 × 0.1 / (0.9 + 0.1) = 0.18
```

This ensures you can't have high F1 with one metric near zero!

## Interpretation

| F1 Score | Interpretation |
|----------|---------------|
| 1.0 | Perfect (precision=1, recall=1) |
| 0.5 | Reasonable |
| 0.0 | Worst (precision=0 or recall=0) |

**Rule of thumb:**
- > 0.9: Excellent
- 0.7-0.9: Good
- 0.5-0.7: Fair
- < 0.5: Poor

## When to Use F1 Score

### Good For:
- **Imbalanced datasets**: When accuracy is misleading
- **Binary classification**: When both precision and recall matter
- **Model comparison**: Single balanced metric
- **Threshold tuning**: Find optimal operating point

### Better Than Accuracy When:
- Classes are imbalanced (e.g., fraud detection, disease detection)
- False positives/negatives have different costs

## Example Calculations

### Scenario 1: High Precision, Low Recall

```
Confusion Matrix:
              Predicted
              Pos   Neg
Actual Pos  [100   400]
Actual Neg  [ 10  490]

Precision = 100/(100+10) = 0.91
Recall = 100/(100+400) = 0.20
F1 = 2 × 0.91 × 0.20 / (0.91 + 0.20) = 0.33
```

**Interpretation**: We found only 20% of positives (low recall), so F1 is low despite high precision.

### Scenario 2: Balanced Precision and Recall

```
Confusion Matrix:
              Predicted
              Pos   Neg
Actual Pos  [400   100]
Actual Neg  [100  1400]

Precision = 400/(400+100) = 0.80
Recall = 400/(400+100) = 0.80
F1 = 2 × 0.80 × 0.80 / (0.80 + 0.80) = 0.80
```

**Interpretation**: Both metrics are good, so F1 is also good.

## Variations: Macro vs Micro vs Weighted

### Macro F1 (Macro-averaged F1)

Calculate F1 for each class independently, then average:

$$F1_{macro} = \frac{1}{K}\sum_{k=1}^{K}F1_k$$

Treats all classes equally regardless of size.

### Micro F1 (Micro-averaged F1)

Aggregate TP, FP, FN across all classes, then compute:

$$F1_{micro} = \frac{2 \times \sum TP}{2 \times \sum TP + \sum FP + \sum FN}$$

Same as accuracy for multi-class!

### Weighted F1 (Weighted-averaged F1)

Calculate F1 for each class, weighted by class frequency:

$$F1_{weighted} = \sum_{k=1}^{K} \frac{n_k}{n} \times F1_k$$

Accounts for class imbalance.

### Example: Multi-class

```
Predictions: [A, A, B, C] vs Actual: [A, B, B, C]

Confusion Matrix:
       Pred A  Pred B  Pred C
Act A[   1       0       0   ]
Act B[   1       1       0   ]
Act C[   0       0       1   ]

F1 per class:
- Class A: precision=1, recall=0.5, F1=0.67
- Class B: precision=0.5, recall=1, F1=0.67
- Class C: precision=1, recall=1, F1=1.00

Macro F1 = (0.67 + 0.67 + 1.0) / 3 = 0.78
```

## Python Implementation

```python
from sklearn.metrics import f1_score, fbeta_score

y_actual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_predicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# F1 Score
f1 = f1_score(y_actual, y_predicted)
print(f"F1 Score: {f1:.2f}")

# Other beta values
# F0.5: More weight on precision
f05 = fbeta_score(y_actual, y_predicted, beta=0.5)

# F2: More weight on recall  
f2 = fbeta_score(y_actual, y_predicted, beta=2)

# Macro and weighted
f1_macro = f1_score(y_actual, y_predicted, average='macro')
f1_weighted = f1_score(y_actual, y_predicted, average='weighted')

print(f"Macro F1: {f1_macro:.2f}")
print(f"Weighted F1: {f1_weighted:.2f}")
```

## Choosing Beta (F-beta)

- **β < 1**: Emphasize precision (fewer false positives)
- **β = 1**: Balance (standard F1)
- **β > 1**: Emphasize recall (fewer false negatives)

**Example:**
- Medical diagnosis: β = 2 (recall more important)
- Recommendation systems: β = 0.5 (precision more important)

## Limitations

### 1. Doesn't Consider True Negatives

F1 focuses only on the positive class. In some cases (like anomaly detection), TN matters.

### 2. Symmetric

For asymmetric problems, you might want different treatment for classes.

### 3. Requires Positive Class Selection

In multi-class, you must decide which class is "positive."

## Interview Questions

**Q: Why is F1 better than accuracy for imbalanced datasets?**
Accuracy can be 99% with 99 negative and 1 positive—predicting all negative. F1 would be 0 because recall is 0, revealing the model doesn't work!

**Q: Why use harmonic mean instead of arithmetic mean?**
Harmonic mean penalizes imbalance. If precision=0.9 and recall=0.1, arithmetic mean is 0.5 (misleading), but harmonic mean is 0.18 (accurate reflection).

**Q: When should you use macro vs weighted F1?**
Use macro when you care equally about all classes. Use weighted when class size matters (common for imbalanced data).

## Summary

- **F1 Score**: Harmonic mean of precision and recall
- **Range**: 0 to 1 (1 is perfect)
- **Balances**: Precision and recall in one metric
- **Best for**: Imbalanced datasets where both false positives and false negatives matter

F1 is the standard metric when you need a single number summarizing classification performance.

## Related Notes

- [[01. Confusion Matrix]]
- [[02. Precision and Recall]]
- [[04. Loss vs Cost Function]]

## Resources

- Lectures: 18-20
