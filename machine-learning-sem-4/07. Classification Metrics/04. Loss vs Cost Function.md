---
tags: [topic]
lecture: [18]
created: 2026-02-13
---

# Loss vs Cost Function

## Overview

The terms "loss function" and "cost function" are often used interchangeably in machine learning, which can cause confusion. While both represent the error to minimize, there's a subtle but important distinction: **loss** applies to a single training example, while **cost** is the aggregate (usually average) across the entire training set.

Understanding this distinction helps avoid common misconceptions and clarifies discussions about optimization.

## The Key Distinction

```
┌─────────────────────────────────────────────┐
│                                             │
│   Loss Function (L)                        │
│   - Measures error for ONE sample           │
│   - Also called "individual loss"           │
│   - Example: (y_i - ŷ_i)²                  │
│                                             │
├─────────────────────────────────────────────┤
│                                             │
│   Cost Function (J)                         │
│   - Measures error over ENTIRE dataset      │
│   - Average (or sum) of all losses          │
│   - Example: (1/n) × Σ(y_i - ŷ_i)²         │
│                                             │
└─────────────────────────────────────────────┘
```

## Loss Function

### Definition

A **loss function** (or simply "loss") measures how wrong a model's prediction is for a **single training example**.

$$L(y_i, \hat{y}_i)$$

Where:
- $y_i$ = actual value for sample $i$
- $\hat{y}_i$ = predicted value for sample $i$

### Examples

| Loss Name | Formula | Use Case |
|-----------|---------|----------|
| **Squared Error** | $(y_i - \hat{y}_i)^2$ | Regression |
| **Absolute Error** | $\|y_i - \hat{y}_i\|$ | Regression |
| **Cross-Entropy** | $-[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$ | Classification |

### Why Per-Sample Loss?

Sometimes we want different weighting for different samples:

```python
# Sample weights in sklearn
sample_weights = [1.0, 1.0, 5.0, 1.0]  # More weight on sample 3
model.fit(X, y, sample_weight=sample_weights)
```

This weights the loss for sample 3 five times more heavily.

## Cost Function

### Definition

A **cost function** (or "objective function") is the **average** (or sum) of the loss across all training examples:

$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}L(y_i, \hat{y}_i)$$

Where:
- $m$ = number of training examples
- $\theta$ = model parameters
- $L$ = loss function

### Examples

| Cost Name | Formula | Description |
|-----------|---------|-------------|
| **Mean Squared Error (MSE)** | $(1/m)\sum(y_i - \hat{y}_i)^2$ | Average squared error |
| **Mean Absolute Error (MAE)** | $(1/m)\sum\|y_i - \hat{y}_i\|$ | Average absolute error |
| **Binary Cross-Entropy** | $-(1/m)\sum[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$ | Log loss for binary classification |

## The Relationship

The cost function is the **aggregate** of the loss function:

```
Loss (single sample):      L(y_i, ŷ_i) = 0.5
Loss (single sample):      L(y_j, ŷ_j) = 1.5
Loss (single sample):      L(y_k, ŷ_k) = 0.8

Cost (average):            J = (0.5 + 1.5 + 0.8) / 3 = 0.93
```

Or in code:

```python
def compute_cost(X, y, weights):
    m = len(y)
    predictions = X @ weights
    losses = (predictions - y) ** 2  # Per-sample loss
    cost = np.mean(losses)            # Average = cost
    return cost
```

## Why Does This Matter?

### 1. Optimization Target

When training, we minimize the **cost function** (the aggregate):

```python
# Gradient descent minimizes COST
for epoch in range(1000):
    cost = compute_cost(X_train, y_train, weights)
    gradients = compute_gradients(X_train, y_train, weights)
    weights = weights - learning_rate * gradients
```

### 2. Reporting Metrics

We report **cost** for training progress:

```python
# Track cost (not loss) per epoch
train_costs = []
for epoch in range(100):
    train_cost = compute_cost(X_train, y_train, weights)
    train_costs.append(train_cost)
    print(f"Epoch {epoch}: Cost = {train_cost:.4f}")
```

### 3. Regularization

Regularization terms are added to the **cost function**:

$$J(\theta) = \text{MSE} + \lambda \sum_{j=1}^{n}\theta_j^2$$

The regularization term is part of the aggregate cost, not individual losses.

## Common Misconceptions

### Misconception 1: "Loss and cost are the same"

**Clarification**: Loss is per-sample, cost is the average. While often used interchangeably in casual conversation, the distinction matters in technical discussions.

### Misconception 2: "Cost must be averaged"

**Clarification**: Some use "cost" to mean the sum rather than average. The important point is consistency—gradient descent works with either as long as you're consistent with the learning rate.

### Misconception 3: "Lower loss means better model"

**Clarification**: You should evaluate on **validation/test cost**, not training loss. A model can have very low training loss but poor generalization (overfitting).

## Python Examples

```python
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

y_actual = np.array([100, 200, 300, 400, 500])
y_predicted = np.array([110, 190, 310, 390, 510])

# Per-sample loss (manual)
individual_losses = (y_actual - y_predicted) ** 2
print(f"Individual losses: {individual_losses}")  # [100, 100, 100, 100, 100]

# Cost function (average)
cost = np.mean(individual_losses)
print(f"Cost (MSE): {cost}")  # 100.0

# Using sklearn
mse = mean_squared_error(y_actual, y_predicted)
mae = mean_absolute_error(y_actual, y_predicted)
print(f"MSE: {mse}, MAE: {mae}")
```

## Interview Questions

**Q: Can you give an example where loss ≠ cost?**
Sure. In a dataset of 1000 samples, suppose we have per-sample squared errors: [1, 1, 1, ..., 1, 10000]. The loss for most samples is 1, but one sample has loss 10000. The cost (average) is ~11, reflecting that one outlier while still showing overall performance.

**Q: Do we minimize loss or cost during training?**
We minimize the cost function. The cost aggregates all individual losses. When we take gradients, we're taking the gradient of the cost (which is the average of individual gradients).

**Q: Why use average instead of sum?**
The scale of the cost affects the learning rate choice. Using average keeps the cost in a reasonable range regardless of dataset size. With sum, you'd need to adjust learning rate based on dataset size.

## Summary

- **Loss**: Error for ONE sample: $L(y_i, \hat{y}_i)$
- **Cost**: Average error over ALL samples: $J = \frac{1}{m}\sum L(y_i, \hat{y}_i)$
- The cost function is what we minimize during training
- Loss and cost are often (confusingly) used interchangeably
- Both must be minimized, but cost is the aggregate objective

## Related Notes

- [[03. F1 Score]]
- [[01. Confusion Matrix]]
- [[04. Loss Functions/01. Loss Functions Concept]]

## Resources

- Lectures: 18-20
