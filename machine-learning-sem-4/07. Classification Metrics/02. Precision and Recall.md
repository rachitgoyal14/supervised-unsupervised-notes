---
tags: [topic]
lecture: [18]
created: 2026-02-13
---

# Precision and Recall

## Overview

**Precision** and **Recall** are two fundamental classification metrics that measure different aspects of model performance. While accuracy tells you the overall correctness, precision and recall reveal how well your model handles specific aspects of classification—especially important when dealing with imbalanced datasets.

Understanding when to prioritize precision vs recall is crucial for making good model decisions in real-world applications.

## Precision

### Definition

Precision measures the accuracy of positive predictions. Of all the samples predicted as positive, how many are actually positive?

$$\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{True Positives}}{\text{All Predicted Positives}}$$

### Intuition

**"Of everything I said was positive, how many were actually positive?"**

- High precision = "When I say positive, I'm usually right"
- Low precision = "I'm over-promising; many are false alarms"

### Examples

**Spam Filter (High Precision = Good):**
- Predicted 10 emails as spam
- 9 were actually spam (TP = 9)
- 1 was legitimate (FP = 1)
- Precision = 9/10 = 90%

**Interpretation**: When marking email as spam, we're right 90% of the time. Low false positives means we rarely bother legitimate emails.

## Recall

### Definition

Recall measures the ability to find all positive samples. Of all the actual positives, how many did we correctly identify?

$$\text{Recall} = \frac{TP}{TP + FN} = \frac{\text{True Positives}}{\text{All Actual Positives}}$$

Also called:
- Sensitivity
- True Positive Rate (TPR)

### Intuition

**"Of all the actual positives, how many did I find?"**

- High recall = "I'm good at finding the positives"
- Low recall = "I'm missing many positives"

### Examples

**Disease Detection (High Recall = Good):**
- 100 patients have disease
- Model correctly identified 85 (TP = 15)
- Model missed 15 (FN = 15)
- Recall = 85/100 = 85%

**Interpretation**: We found 85% of the sick patients. We're missing 15%—those patients won't get treatment!

## Precision vs Recall Trade-off

There's often a trade-off between precision and recall:

```
                    Predicted
              ┌───────────────────┐
              │   YES    │   NO   │
    ┌─────────┼──────────┼────────┤
 YES│         │    ✓     │   ✗    │
    │         │   (TP)   │  (FN)  │
    ├─────────┼──────────┼────────┤
 NO │         │    ✗     │   ✓    │
    │         │   (FP)   │  (TN)  │
    └─────────┴──────────────────┘
```

**Why Trade-off Exists:**

1. **Lowering threshold**: Predict more as positive
   - More TP found (higher recall)
   - But also more FP (lower precision)

2. **Raising threshold**: Predict fewer as positive
   - Fewer FP (higher precision)
   - But also miss more TP (lower recall)

### Visual Intuition

```
        Precision
          │
    1.0   │     ╲
          │       ╲
          │         ╲
    0.5   │           ╲_______ Recall
          │             ╱
          │           ╱
          │         ╱
    0.0   │_______╱___________
               Threshold
               
    As threshold changes:
    - High threshold → High precision, Low recall
    - Low threshold → Low precision, High recall
```

## When to Prioritize Precision

Use **precision** when:
- **False positives are costly**: Spam filter, fraud detection
- **Being right matters more than finding everything**: Recommendation systems
- **High confidence is required**: Medical diagnosis confirmation

**Examples:**
- **Spam filter**: Don't want to accidentally delete important emails
- **Fraud detection**: Blocking transactions should be correct
- **Hiring screening**: Don't want to waste interview time

## When to Prioritize Recall

Use **recall** when:
- **False negatives are costly**: Disease detection, security
- **Finding everything matters**: Cancer screening, defect detection
- **Missing positives is unacceptable**: Threat detection

**Examples:**
- **Cancer screening**: Better to have false alarms than miss cancer
- **Security systems**: Better to alert on harmless activity than miss threats
- **Safety systems**: Fault detection should catch all issues

## Real-World Example

### Cancer Detection System

Let's analyze a model's confusion matrix:

```
Predicted: Cancer    Predicted: Healthy
                │
Actual: Cancer  │   80 (TP)     │  20 (FN)  │
Actual: Healthy │   10 (FP)     │ 890 (TN)  │
                └──────────────┘
```

| Metric | Formula | Value |
|--------|---------|-------|
| **Precision** | 80/(80+10) | 88.9% |
| **Recall** | 80/(80+20) | 80% |

**Interpretation:**
- 88.9% of people flagged as cancer actually have cancer
- 80% of all cancer cases were detected

## Python Implementation

```python
from sklearn.metrics import precision_score, recall_score, confusion_matrix
import numpy as np

y_actual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_predicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# Calculate metrics
precision = precision_score(y_actual, y_predicted)
recall = recall_score(y_actual, y_predicted)

print(f"Precision: {precision:.2f}")  # 0.80
print(f"Recall: {recall:.2f}")       # 0.80

# Confusion matrix
cm = confusion_matrix(y_actual, y_predicted)
print(cm)

# Extract values
TN, FP, FN, TP = cm.ravel()
```

## Selecting Threshold

```python
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Get prediction probabilities
y_proba = model.predict_proba(X_test)[:, 1]

# Calculate precision and recall at various thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision-Recall vs Threshold')
plt.legend()
plt.grid()
plt.show()
```

## Interview Questions

**Q: What if precision and recall are both low?**
The model isn't learning the positive class well. Check data quality, feature engineering, or try different algorithms.

**Q: Can precision or recall be greater than 1?**
No, both are bounded between 0 and 1.

**Q: Why is there often a trade-off between precision and recall?**
Because adjusting the decision threshold affects both. Lowering threshold → predict more positives → higher recall, lower precision. Raising threshold → predict fewer positives → higher precision, lower recall.

**Q: In a cancer detection test, would you prefer high precision or high recall?**
Generally high recall is more important—we want to catch as many cancer cases as possible, even at the cost of some false alarms. Missing a cancer case (false negative) is far more serious than a false alarm (false positive).

## Summary

- **Precision**: Of predicted positives, how many are correct? (Quality of positive predictions)
- **Recall**: Of actual positives, how many did we find? (Coverage of positive class)
- **Trade-off**: Can't maximize both simultaneously; choose based on costs
- **Key insight**: Use context to decide which matters more—missing positives or false alarms

## Related Notes

- [[01. Confusion Matrix]]
- [[03. F1 Score]] (combines precision and recall)
- [[04. Loss vs Cost Function]]

## Resources

- Lectures: 18-20
