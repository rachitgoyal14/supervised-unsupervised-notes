---
tags: [topic]
lecture: [8]
created: 2026-02-13
---

# Best Line Math

## Overview

While gradient descent provides an iterative approach to finding the best fit line, there exists a closed-form (direct) solution called the **Normal Equation**. This mathematical derivation gives us the exact optimal values for slope and intercept in one calculation, without any iterations. Understanding this math deepens our intuition for what "best fit" truly means.

The key insight is that we can use calculus to find where the cost function (MSE) is minimized by setting its derivatives to zero and solving for the parameters directly.

## Derivation for Simple Linear Regression

For simple linear regression ($y = \beta_0 + \beta_1 x$), we want to find the slope ($\beta_1$) and intercept ($\beta_0$) that minimize MSE.

### The Cost Function

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - (\beta_0 + \beta_1 x_i))^2$$

### Finding the Optimal Slope ($\beta_1$)

To find the optimal slope, we take the partial derivative of MSE with respect to $\beta_1$ and set it to zero:

$$\frac{\partial}{\partial \beta_1} MSE = 0$$

After differentiation and algebraic manipulation, we get:

$$\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$

Where:
- $\bar{x}$ = mean of x values
- $\bar{y}$ = mean of y values

This formula tells us to compute the covariance of x and y, divided by the variance of x.

### Finding the Optimal Intercept ($\beta_0$)

Similarly, taking the partial derivative with respect to $\beta_0$:

$$\beta_0 = \bar{y} - \beta_1 \bar{x}$$

This makes intuitive sense: the intercept is where the line crosses the y-axis when x is at its mean.

## The Normal Equation (Matrix Form)

For multiple linear regression with multiple features, we use matrix notation. The Normal Equation is:

$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

Where:
- $\hat{\beta}$ = vector of optimal coefficients (including intercept)
- $X$ = design matrix (features with bias column)
- $y$ = target vector
- $X^T$ = transpose of X
- $(X^TX)^{-1}$ = inverse of $X^TX$

### Matrix Dimensions

If we have $m$ samples and $n$ features:
- $X$ is $m \times (n+1)$ (extra column for bias = 1)
- $y$ is $m \times 1$
- $(X^TX)$ is $(n+1) \times (n+1)$
- $\hat{\beta}$ is $(n+1) \times 1$

## Computational Complexity

The main drawback of the Normal Equation is computational complexity:

- Matrix multiplication: $O(m \times n^2)$
- Matrix inversion: $O(n^3)$

For large numbers of features ($n$), this becomes prohibitively expensive.

| Features (n) | Approximate Time |
|--------------|-----------------|
| 10 | < 1 ms |
| 100 | ~10 ms |
| 1,000 | ~10 seconds |
| 10,000 | ~3 hours |
| 100,000 | Impractical |

## Python Implementation: Normal Equation

```python
import numpy as np

def normal_equation(X, y):
    """
    Solve linear regression using the Normal Equation
    Î² = (X^T X)^(-1) X^T y
    """
    # Add bias column (column of ones) for intercept
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    
    # Compute coefficients using normal equation
    beta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
    
    return beta

# Example: Simple linear regression
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

coefficients = normal_equation(X, y)
print(f"Intercept: {coefficients[0]:.2f}")  # ~0.0
print(f"Slope: {coefficients[1]:.2f}")     # ~2.0
```

## Python Implementation: Scikit-Learn

```python
from sklearn.linear_model import LinearRegression

# Create and fit model
model = LinearRegression()
model.fit(X, y)

print(f"Intercept: {model.intercept_}")  # 0.0
print(f"Coefficient: {model.coef_[0]}") # 2.0
```

## Comparison: Normal Equation vs Gradient Descent

| Aspect | Normal Equation | Gradient Descent |
|--------|-----------------|------------------|
| **Type** | Closed-form | Iterative |
| **Computation** | One step | Many steps |
| **Complexity** | $O(n^3)$ | $O(k \cdot m \cdot n)$ |
| **Large n** | Slow | Fast |
| **Out of core** | No | Yes |
| **Hyperparameters** | None | Learning rate |

## When to Use Each

**Use Normal Equation when:**
- Number of features < 10,000
- Dataset fits in memory
- You need exact solution
- Quick prototyping

**Use Gradient Descent when:**
- Many features (> 10,000)
- Large dataset
- Memory is limited
- Using regularization

## The Intuition Behind the Math

Think of the slope formula:

$$\beta_1 = \frac{\sum(x - \bar{x})(y - \bar{y})}{\sum(x - \bar{x})^2}$$

- **Numerator**: How x and y move together (covariance)
- **Denominator**: How much x varies on its own (variance)

The ratio tells us: "For every unit that x varies from its mean, how many units does y vary from its mean?"

If x and y are positively correlated, the numerator is positive â†’ positive slope.
If x and y are negatively correlated, the numerator is negative â†’ negative slope.

## Interview Questions

**Q: Why does the normal equation require inverting a matrix?**
The inverse operation solves the system of normal equations. When $X^TX$ is singular (non-invertible), we can't use the normal equation. This happens with multicollinearity or when n > m.

**Q: What happens if $X^TX$ is not invertible?**
The normal equation fails. Solutions include: removing correlated features, using regularization (Ridge Regression), or switching to gradient descent.

## ðŸ“– From Class Notes

### Step-by-Step Derivation from Lecture

The class notes provide a detailed derivation of the slope formula. Here's the step-by-step process:

**Step 1: Define the sum to minimize**
$$S = \sum_{i=1}^{n}[y_i - m \cdot x_i - c]^2$$

**Step 2: Take partial derivative with respect to c and set to 0:**
$$\frac{\partial S}{\partial c} = \sum 2 \cdot [y_i - m \cdot x_i - c] \cdot (-1) = 0$$

This simplifies to:
$$\sum y_i - m \sum x_i - n \cdot c = 0$$

**Step 3: Take partial derivative with respect to m and set to 0:**
$$\frac{\partial S}{\partial m} = \sum 2 \cdot [y_i - m \cdot x_i - c] \cdot (-x_i) = 0$$

This simplifies to:
$$\sum x_i \cdot y_i - m \sum x_i^2 - c \sum x_i = 0$$

**Step 4: Solve for m (slope):**
After substituting c from Step 2 back into Step 3 and simplifying:
$$m = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$

### Numerical Example from Class

Using synthetic data y = 2x + 5 + noise:
- X = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
- y = [5.64, 7.14, 8.88, 10.91, 12.83, 15.03, 17.69, 18.84, 21.01, 23.59]

Results from class:
- x_mean = 4.5
- y_mean = 14.16
- Slope (m) = 2.01
- Intercept (c) = 5.13

This is very close to the true values (m=2, c=5), demonstrating that OLS successfully recovers the underlying relationship even with noise.

### np.polyfit() Function

From class: The NumPy function `np.polyfit(x, y, degree)` finds the best-fit polynomial coefficients. For linear regression, use degree=1.

```python
import numpy as np
coefficients = np.polyfit(x_data, y_data, 1)
# Returns [slope, intercept]
```

---

## Summary

The Normal Equation provides a direct mathematical solution to linear regression:
- Slope: $\beta_1 = \frac{\sum(x - \bar{x})(y - \bar{y})}{\sum(x - \bar{x})^2}$
- Intercept: $\beta_0 = \bar{y} - \beta_1 \bar{x}$
- Multiple features: $\hat{\beta} = (X^TX)^{-1}X^Ty$

While elegant, it doesn't scale to large feature sets due to $O(n^3)$ complexity.

## Related Notes

- [[02. Simple Linear Regression]]
- [[03. Best Line Concept]]
- [[04. Best Line Metrics]]
- [[05. Gradient Descent/02. Closed Form vs Gradient Descent]]

## Resources

- Lectures: 8-9
- Notebooks: `../../linear-regression/`
- Book: "Hands-On Machine Learning" Chapter 4
