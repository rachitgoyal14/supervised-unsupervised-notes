---
tags: [topic]
lecture: [8]
created: 2026-02-13
---

# Best Line Concept

## Overview

When we fit a linear regression model, we're essentially trying to draw the "best" straight line through our data points. But what exactly makes one line better than another? The concept of "best fit" is fundamental to understanding linear regression. Rather than arbitrarily drawing lines through data, we need a systematic way to measure and minimize the error between our predictions and actual values.

The key insight is that we can quantify "goodness of fit" using a cost function—a mathematical measure of how wrong our predictions are. By minimizing this cost function, we find the optimal line that best represents the underlying relationship between variables.

## What Makes a Line the "Best Fit"?

The best fit line is the one that **minimizes the total prediction error** across all training examples. We define "error" as the difference between what our model predicts ($\hat{y}$) and what the actual value is ($y$):

$$\text{Error} = y - \hat{y}$$

A perfect model would have zero error for all points, but this is rarely achievable (and would indicate overfitting). The goal is to find the line that makes the smallest possible errors on average.

## Visual Intuition

Consider this scatter plot with multiple possible lines:

```
     y
     │        ●        ●
     │      ●    ●   ●
     │    ●        ●
     │  ●          ●●
     │●              ●
     │________________x
     
     The "best" line minimizes total
     distance from all points to the line
```

Different lines have different total errors. The best line passes through the data in a way that balances over- and under-predictions across all points.

## The Problem with Simple Error Sum

A naive approach would be to sum up all errors:

$$\text{Total Error} = \sum_{i=1}^{n}(y_i - \hat{y}_i)$$

However, this doesn't work! Positive and negative errors cancel each other out. Consider:

- Point 1: $y = 10$, $\hat{y} = 12$, error = -2
- Point 2: $y = 10$, $\hat{y} = 8$, error = +2

Total Error = -2 + 2 = 0

This would suggest a perfect model even though both predictions are wrong!

## Solution: Square the Errors

To prevent cancellation, we square each error before summing:

$$\text{SSE} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

This is called the **Sum of Squared Errors (SSE)**. Now:
- Both errors become positive: (-2)² = 4, (+2)² = 4
- Total = 4 + 4 = 8 (we can see the model makes mistakes!)

Additionally, squaring penalizes larger errors more heavily:
- Error of 2 → squared = 4
- Error of 10 → squared = 100 (25x more penalty!)

This encourages the model to avoid large mistakes.

## The Cost Function

For linear regression, we minimize the **Mean Squared Error (MSE)**:

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

This is our **cost function**—the metric we optimize to find the best line.

## How the Best Line is Found

The process involves:
1. **Initialization**: Start with arbitrary values for slope ($m$) and intercept ($c$)
2. **Prediction**: Calculate $\hat{y} = mx + c$ for each point
3. **Error Calculation**: Compute MSE using predictions vs actual values
4. **Update**: Adjust $m$ and $c$ to reduce MSE
5. **Repeat**: Continue until MSE stops improving (convergence)

This iterative optimization is called **gradient descent**.

## Why This Works

The MSE cost function is **convex**—it has a single global minimum with no local minima. This means there's only one "valley" to find, and gradient descent will find the bottom (optimal line) regardless of where we start.

```
        MSE
         │
         │       ╱⎺⎺╲
         │     ╱      ╲
         │   ╱          ╲
         │ ╱              ╲
         │╱________________╲___
                    Best Line
                    
    Only one minimum = guaranteed convergence
```

## Related Notes

- [[01. Linear Regression Ideal]]
- [[02. Simple Linear Regression]]
- [[04. Best Line Metrics]]
- [[05. Best Line Math]]

## Resources

- Lectures: 8-9
