---
tags: [topic]
lecture: [8]
created: 2026-02-13
---

# Best Line Metrics

## Overview

Once we've fit a linear regression model, we need to evaluate how well it performs. Various metrics exist to quantify model quality, each with different interpretations and use cases. Understanding these metrics is essential for comparing models, diagnosing problems, and choosing the best approach for your data.

The most common regression metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R²). Each metric emphasizes different aspects of model performance—some penalize large errors more heavily, while others measure relative rather than absolute performance.

## Mean Squared Error (MSE)

### Definition

MSE measures the average squared difference between predicted and actual values:

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

### Key Properties

- **Unit**: Squared units of the target variable
- **Range**: 0 to ∞ (lower is better)
- **Sensitivity**: Highly sensitive to outliers due to squaring

### When to Use

Use MSE when large errors are particularly problematic and you want to penalize outliers heavily.

### Python Example

```python
from sklearn.metrics import mean_squared_error
import numpy as np

y_actual = np.array([100, 200, 300, 400, 500])
y_predicted = np.array([110, 190, 310, 390, 510])

mse = mean_squared_error(y_actual, y_predicted)
print(f"MSE: {mse}")  # 200.0
```

## Root Mean Squared Error (RMSE)

### Definition

RMSE is the square root of MSE, bringing the metric back to original units:

$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

### Key Properties

- **Unit**: Same as the target variable (interpretable!)
- **Range**: 0 to ∞ (lower is better)
- **Interpretation**: Typical prediction error magnitude

### When to Use

Use RMSE when you need interpretable error values in the same units as your target variable.

### Python Example

```python
from sklearn.metrics import mean_squared_error
import numpy as np

y_actual = np.array([100, 200, 300, 400, 500])
y_predicted = np.array([110, 190, 310, 390, 510])

rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))
print(f"RMSE: {rmse}")  # 14.14
```

## Mean Absolute Error (MAE)

### Definition

MAE measures the average absolute difference between predicted and actual values:

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

### Key Properties

- **Unit**: Same as the target variable
- **Range**: 0 to ∞ (lower is better)
- **Sensitivity**: Treats all errors equally, robust to outliers

### When to Use

Use MAE when outliers should not dominate the error metric, or when direction of error doesn't matter.

### Comparison: MAE vs MSE vs RMSE

| Error | MAE | Squared | Contribution to MSE |
|-------|-----|---------|-------------------|
| 1 | 1 | 1 | Low |
| 5 | 5 | 25 | Moderate |
| 10 | 10 | 100 | High |

MSE/RMSE penalize large errors much more heavily than MAE.

## R-Squared (R²)

### Definition

R² measures the proportion of variance in the target variable explained by the model:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

Where:
- $SS_{res}$ = Sum of squared residuals (model error)
- $SS_{tot}$ = Total sum of squares (variance from mean)
- $\bar{y}$ = Mean of actual values

### Key Properties

- **Range**: -∞ to 1 (higher is better)
- **Interpretation**: Percentage of variance explained

### Interpretation Guide

| R² Value | Interpretation |
|----------|---------------|
| 1.0 | Perfect prediction |
| 0.9 | Model explains 90% of variance |
| 0.5 | Model explains 50% of variance |
| 0.0 | Model predicts the mean |
| < 0 | Worse than predicting the mean |

### When to Use

Use R² when you want to understand how much of the target's variability your model captures. It's the most commonly reported metric.

### Python Example

```python
from sklearn.metrics import r2_score
import numpy as np

y_actual = np.array([100, 200, 300, 400, 500])
y_predicted = np.array([110, 190, 310, 390, 510])

r2 = r2_score(y_actual, y_predicted)
print(f"R²: {r2}")  # 0.96
```

## Choosing the Right Metric

| Metric | Best For | Sensitive to Outliers |
|--------|----------|---------------------|
| **MSE** | Penalizing large errors | Yes (very) |
| **RMSE** | Interpretable error magnitude | Yes |
| **MAE** | Robust to outliers | No |
| **R²** | Understanding variance explained | Relative |

## Summary

- **MSE**: Penalizes large errors heavily; good when large errors are costly
- **RMSE**: Interpretable in original units; the most commonly used metric
- **MAE**: Robust to outliers; treats all errors equally
- **R²**: Measures explanatory power; standard for reporting model quality

## Related Notes

- [[02. Simple Linear Regression]]
- [[03. Best Line Concept]]
- [[05. Best Line Math]]
- [[04. Loss Functions/02. Mean Squared Error]]
- [[04. Loss Functions/03. Root Mean Squared Error]]
- [[04. Loss Functions/04. Mean Absolute Error]]

## Resources

- Lectures: 8-9
- Notebooks: `../../linear-regression/`
