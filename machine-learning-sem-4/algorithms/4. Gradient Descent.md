

explain the whole situation with gradient descent and also explain stuff like osciallations, saddle point etc, include formula and do it for regression only




there are 3 types of gradient descent:

1. batch gradient descent


in one iteration, we cover the whole dataset to cover the partial derivative or `m `& `c`

advantages:
small data 
low number of features is where this is applicable

It moves directly towards the minimum (smooth curve)
It finds the exact mathematical minimum (for convex problems)

Disadvantages:
1. slow: extremely computationally expensive for large datasets
2. memory heavy: requires loading all data into memory at once hence its use is limited to small datasets


3. stochastic gradient descent (SGD)
there is an analogy to understand this
there are 500 students who are taught 10 topics and students have certain doubts in every topic
now like if we do it in every iteration and deal with every student, like 500 students in total, then we get first updated parameters
then we get new m and c in the 2nd iteration
and doing this for like 10 times gives me a new value of `m` and `c`, which give me precise values and generally best idea of what actually is happening
this is the batch gradient descent

now lets have a look at SGD
out of 500 students, we pick one student randomly and then we calculate the value of m and c for a single iteration
now for the 2nd iteration, we do it once again by identifying a random student and calculate new values of `m` and `c`

here are the advantages of this:

1. computationally easy
2. fast

disadvantages:

1. preciseness
2. 