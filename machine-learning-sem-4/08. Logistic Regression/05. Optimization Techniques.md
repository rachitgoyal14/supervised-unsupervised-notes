---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Optimization Techniques

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## Why No Closed-Form Solution?

Unlike linear regression, which has a **closed-form solution** (the normal equation $\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$), logistic regression does not have a simple analytical solution.

The reason is that the log loss function involves the sigmoid, which introduces nonlinearity. While setting the gradient to zero yields an equation, it cannot be solved analytically for the weights.

This is why we rely on **iterative optimization algorithms** like gradient descent.

## Gradient Descent for Logistic Regression

The gradient of the log loss cost function with respect to each weight $w_j$ is:

$$\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \left( \sigma(\mathbf{w}^T \mathbf{x}^{(i)}) - y^{(i)} \right) x_j^{(i)}$$

### Weight Update Rule

For each iteration $t$:

$$w_j^{(t+1)} = w_j^{(t)} - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} \left( p^{(i)} - y^{(i)} \right) x_j^{(i)}$$

where $\eta$ is the learning rate.

This update rule is remarkably similar to linear regression's gradient descent — the only difference is that the prediction $p^{(i)}$ comes from the sigmoid function rather than a direct linear computation.

### Gradient Descent Variants

| Method | Description | Pros | Cons |
|--------|-------------|------|------|
| **Batch GD** | Uses all $m$ samples per update | Stable convergence | Slow for large datasets |
| **Stochastic GD** | Uses 1 sample per update | Fast, can escape local minima | Noisy updates |
| **Mini-batch GD** | Uses $k$ samples per update ($1 < k < m$) | Balance of speed and stability | Requires tuning batch size |

## Learning Rate Selection

The learning rate $\eta$ is crucial for convergence:

- **Too small**: Converges slowly, may get stuck in shallow local minima
- **Too large**: Overshoots minimum, may diverge or oscillate
- **Adaptive learning rates**: Some solvers (like `lbfgs`) automatically adjust the step size

### Convergence Criteria

Common stopping conditions:
- Maximum iterations reached (`max_iter`)
- Gradient magnitude below threshold (norm of gradient < tolerance)
- Improvement in loss below threshold

## Solvers in scikit-learn

scikit-learn's `LogisticRegression` supports multiple solvers:

| Solver | Description | Best For |
|--------|-------------|----------|
| **`lbfgs`** | Limited-memory Broyden–Fletcher–Goldfarb–Shanno | Default, medium datasets |
| **`saga`** | Stochastic Average Gradient | Large datasets, L1/L2 penalty |
| **`liblinear`** | Library for Large Linear Classification | Small datasets, L1 penalty |
| **`sag`** | Stochastic Average Gradient | Large datasets (slower than SAGA) |
| **`newton-cg`** | Newton-CG | Medium datasets, L2 penalty |

### Choosing a Solver

```python
from sklearn.linear_model import LogisticRegression

# Default: lbfgs solver
model = LogisticRegression(solver='lbfgs', max_iter=1000)

# For L1 penalty (sparse model)
model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)

# For large dataset with L2 penalty
model = LogisticRegression(solver='saga', max_iter=1000)

# For elastic net (L1 + L2)
model = LogisticRegression(penalty='elasticnet', solver='saga', 
                         l1_ratio=0.5, max_iter=1000)
```

## Code Example

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42)

# Train with different solvers
solvers = ['lbfgs', 'liblinear', 'saga', 'newton-cg']

for solver in solvers:
    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)
    model.fit(X, y)
    accuracy = model.score(X, y)
    print(f"{solver}: Accuracy = {accuracy:.4f}")

# Custom gradient descent implementation
def gradient_descent_logistic(X, y, lr=0.1, max_iter=1000, tol=1e-6):
    m, n = X.shape
    X = np.c_[np.ones(m), X]  # Add bias term
    weights = np.zeros(n + 1)
    
    for i in range(max_iter):
        z = X @ weights
        predictions = 1 / (1 + np.exp(-z))
        gradients = X.T @ (predictions - y) / m
        
        if np.linalg.norm(gradients) < tol:
            print(f"Converged at iteration {i}")
            break
            
        weights -= lr * gradients
    
    return weights

weights = gradient_descent_logistic(X, y, lr=0.1, max_iter=1000)
print(f"Learned weights: {weights}")
```

## Topics to Cover

- Gradient descent for logistic regression
- Learning rate selection
- Convergence criteria

## Related Notes

- [[04. Loss Function]]
- [[05. Gradient Descent/03. Gradient Descent Introduction]]

## Resources

- Lectures: 21-24
- Hands-On Machine Learning (Géron), Chapter 4

---

## Summary

Logistic regression lacks a closed-form solution due to the nonlinear sigmoid function, requiring iterative optimization. Gradient descent updates weights by computing the gradient of log loss, with batch, stochastic, and mini-batch variants offering different tradeoffs. scikit-learn provides multiple solvers (lbfgs, saga, liblinear) optimized for different dataset sizes and penalty types, with convergence controlled by `max_iter` and tolerance parameters.

