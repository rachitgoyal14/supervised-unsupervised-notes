---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Step Function

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## What is the Step Function?

The **step function** (also called the **Heaviside step function**) is the simplest threshold-based classifier. It maps any real number to a binary output:

$$
f(z) = 
\begin{cases} 
1 & \text{if } z > 0 \\
0 & \text{if } z \leq 0 
\end{cases}
$$

where $z = w_0 + w_1x_1 + w_2x_2 + \dots$ is the weighted sum of input features.

## Step Function in the Perceptron Model

The **perceptron** is a simple neural network unit that uses the step function as its activation. For a dataset with two features, the decision boundary is a straight line defined by:

$$w_0 + w_1x_1 + w_2x_2 = 0$$

This can be rewritten as:

$$y = w_0 \cdot x_0 + w_1 \cdot x_1 + w_2 \cdot x_2 = 0$$

where $x_0 = 1$ (the bias term).

The perceptron computes $z = \sum w_i x_i$ and applies the step function:

- If $z > 0$ → predict **class 1**
- If $z \leq 0$ → predict **class 0**

### Code Example

```python
import numpy as np

def step_function(z):
    return np.where(z > 0, 1, 0)

# Test the step function
test_values = np.array([-2, -0.5, 0, 0.5, 2])
print(step_function(test_values))
# Output: [0 0 0 1 1]
```

## Limitations of the Step Function

Despite its simplicity, the step function has several significant drawbacks:

1. **No probability output**: It provides only hard 0/1 classifications with no confidence measure
2. **Not differentiable**: The step function has a discontinuity at $z = 0$, making it impossible to use with gradient descent optimization
3. **Binary updates**: Weights are only updated when a point is misclassified, which can lead to slow or unstable learning
4. **Sensitive to outliers**: Noisy or extreme data points can drastically affect the decision boundary

## Why Sigmoid is Preferred

The **sigmoid function** (covered in [[03. Sigmoid Function]]) addresses these limitations:

- Outputs values between 0 and 1 (interpretable as probabilities)
- Is smoothly differentiable everywhere
- Provides gradient information for every misclassified point
- Is less sensitive to outliers due to its bounded output

## Topics to Cover

- Step function definition
- Binary threshold
- Relationship to perceptron

## Related Notes

- [[01. Logistic Regression]]
- [[03. Sigmoid Function]]

## Resources

- Lectures: 21-24

---

## Summary

The step function is a binary threshold activation that maps any weighted input to either 0 or 1. While it forms the basis of the perceptron model for understanding decision boundaries, its non-differentiability and lack of probabilistic output make it unsuitable for gradient-based optimization. The sigmoid function resolves these issues, enabling efficient training via gradient descent.

