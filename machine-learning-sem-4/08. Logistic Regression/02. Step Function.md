---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Step Function

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## What is the Step Function?

The **step function** (also called the **Heaviside step function**) is the simplest threshold-based classifier. It maps any real number to a binary output:

$$
f(z) = 
\begin{cases} 
1 & \text{if } z > 0 \\
0 & \text{if } z \leq 0 
\end{cases}
$$

where $z = w_0 + w_1x_1 + w_2x_2 + \dots$ is the weighted sum of input features.

## Step Function in the Perceptron Model

The **perceptron** is a simple neural network unit that uses the step function as its activation. For a dataset with two features, the decision boundary is a straight line defined by:

$$w_0 + w_1x_1 + w_2x_2 = 0$$

This can be rewritten as:

$$y = w_0 \cdot x_0 + w_1 \cdot x_1 + w_2 \cdot x_2 = 0$$

where $x_0 = 1$ (the bias term).

The perceptron computes $z = \sum w_i x_i$ and applies the step function:

- If $z > 0$ → predict **class 1**
- If $z \leq 0$ → predict **class 0**

### The Perceptron Trick

The **Perceptron Trick** is the geometric way of understanding how the decision boundary moves when a point is misclassified. For 2 features, if bias is included inside the summation (by taking $x_0 = 1$):

$$z = w_0 x_0 + w_1 x_1 + w_2 x_2$$

The prediction rule using the step function is:

- Output = **1** if $z > 0$
- Output = **0** if $z \leq 0$

When a point is misclassified, the perceptron updates its weights to shift the decision boundary toward the correct side. This is the geometric interpretation of the weight update rule — the hyperplane rotates until the point is correctly classified.

### Code Example

```python
import numpy as np

def step_function(z):
    return np.where(z > 0, 1, 0)

# Test the step function
test_values = np.array([-2, -0.5, 0, 0.5, 2])
print(step_function(test_values))
# Output: [0 0 0 1 1]
```

## Limitations of the Step Function

Despite its simplicity, the step function has several significant drawbacks:

1. **No probability output**: It provides only hard 0/1 classifications with no confidence measure. There is no way to express "70% likely class 1" — it's always a binary decision.
2. **Not differentiable**: The step function has a discontinuity at $z = 0$, making it impossible to use with gradient descent optimization. There is no gradient to propagate.
3. **Binary updates**: Weights are only updated when a point is misclassified, which can lead to slow or unstable learning.
4. **Sensitive to outliers**: Noisy or extreme data points can drastically affect the decision boundary.
5. **No probabilistic interpretation**: The output cannot be interpreted as a probability, unlike the sigmoid function.

## Why Sigmoid is Preferred

The **sigmoid function** (covered in [[03. Sigmoid Function]]) addresses these limitations:

- Outputs values between 0 and 1 (interpretable as probabilities)
- Is smoothly differentiable everywhere
- Provides gradient information for every misclassified point
- Is less sensitive to outliers due to its bounded output
- Enables the use of **gradient descent** for optimization

The sigmoid function can be seen as a "soft" version of the step function — instead of jumping abruptly from 0 to 1 at $z = 0$, it transitions smoothly through the S-shaped curve, providing a continuous measure of confidence.

## Topics to Cover

- Step function definition
- Binary threshold
- Relationship to perceptron

## Related Notes

- [[01. Logistic Regression]]
- [[03. Sigmoid Function]]

## Resources

- Lectures: 21-24

---

## Summary

The step function is a binary threshold activation that maps any weighted input to either 0 or 1. While it forms the basis of the perceptron model for understanding decision boundaries, its non-differentiability and lack of probabilistic output make it unsuitable for gradient-based optimization. The sigmoid function resolves these issues, enabling efficient training via gradient descent.