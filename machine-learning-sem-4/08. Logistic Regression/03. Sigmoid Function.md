---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Sigmoid Function

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## Why Sigmoid?

The **step function** (covered in [[02. Step Function]]) provides only hard 0/1 outputs with no sense of confidence. The **sigmoid function** (also called the **logistic function**) solves this by producing a smooth, continuous output between 0 and 1 that can be directly interpreted as a probability.

## The Sigmoid Formula

The sigmoid function is defined as:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

where $z = w_0 + w_1x_1 + w_2x_2 + \dots$ is the linear combination of weights and input features.

### Breaking Down the Formula

- **$e^{-z}$**: The exponential term ensures the denominator is always > 1
- **When $z \to \infty$**: $e^{-z} \to 0$, so $\sigma(z) \to 1$
- **When $z \to -\infty$**: $e^{-z} \to \infty$, so $\sigma(z) \to 0$
- **When $z = 0$**: $\sigma(0) = 1 / (1 + 1) = 0.5$

## Properties of the Sigmoid Curve

1. **Output range**: Always between 0 and 1
2. **S-shaped curve**: Monotonically increasing, steep in the middle, flatter at extremes
3. **Symmetric**: $\sigma(-z) = 1 - \sigma(z)$
4. **Decision boundary at $z = 0$**: Where output equals 0.5
5. **Gradient property**: Smoothly differentiable everywhere

## Computing $z$

The value $z$ is the weighted sum of inputs plus bias:

$$z = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n$$

In vector form: $z = \mathbf{w}^T \mathbf{x}$

## Decision Rule

After computing $p = \sigma(z)$, we classify as follows:

- If $p \geq 0.5$ → predict **class 1**
- If $p < 0.5$ → predict **class 0**

This is equivalent to classifying based on whether $z \geq 0$.

### Geometric Interpretation

The equation $z = 0$ defines the **decision boundary** — the hyperplane that separates the two classes. Points on one side have $z > 0$ (probability > 0.5), and points on the other have $z < 0$ (probability < 0.5).

## Differentiability

The sigmoid function is differentiable everywhere, which is crucial for gradient descent optimization:

$$\frac{d\sigma(z)}{dz} = \sigma(z)(1 - \sigma(z))$$

This derivative can be computed using only the output $\sigma(z)$, making backpropagation computationally efficient.

## Code Example

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Test sigmoid on a range of z values
z_values = np.linspace(-10, 10, 21)
print("z values and sigmoid outputs:")
for z in z_values:
    print(f"z = {z:5.1f}  →  σ(z) = {sigmoid(z):.4f}")

# Plot the sigmoid curve
z_plot = np.linspace(-10, 10, 100)
plt.plot(z_plot, sigmoid(z_plot))
plt.axhline(y=0.5, color='r', linestyle='--', label='Decision threshold')
plt.axvline(x=0, color='g', linestyle='--', label='Decision boundary')
plt.xlabel('z')
plt.ylabel('σ(z)')
plt.title('Sigmoid Function')
plt.legend()
plt.grid(True)
plt.show()
```

Output:
```
z values and sigmoid outputs:
z =  -10  →  σ(z) = 0.0000
z =   -8  →  σ(z) = 0.0003
z =   -6  →  σ(z) = 0.0025
z =   -4  →  σ(z) = 0.0180
z =   -2  →  σ(z) = 0.1192
z =    0  →  σ(z) = 0.5000
z =    2  →  σ(z) = 0.8808
z =    4  →  σ(z) = 0.9820
z =    6  →  σ(z) = 0.9975
z =    8  →  σ(z) = 0.9997
z =   10  →  σ(z) = 1.0000
```

## Topics to Cover

- Sigmoid curve
- Probability interpretation
- Gradient properties

## Related Notes

- [[02. Step Function]]
- [[04. Loss Function]]

## Resources

- Lectures: 21-24

---

## Summary

The sigmoid function maps any real number to the (0, 1) interval, enabling probabilistic interpretation of classification outputs. Its S-shaped curve provides smooth transitions between classes, with the decision boundary at z = 0 (p = 0.5). The key advantage over the step function is its differentiability, which allows gradient descent to efficiently optimize the model's weights during training.

