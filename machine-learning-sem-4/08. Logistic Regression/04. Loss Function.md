---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Loss Function

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## Why Not MSE?

Using **Mean Squared Error (MSE)** for logistic regression creates several problems:

### Non-Convex Loss Surface

The MSE loss function for logistic regression is **non-convex** — it has multiple local minima and maxima. When using gradient descent, the optimization algorithm can get stuck in these local minima rather than finding the global optimum.

### Inappropriate Penalty Structure

MSE penalizes confident wrong predictions proportionally to the squared error, but this doesn't align with how we want to train classifiers. A confident wrong prediction should be penalized more heavily than an uncertain one.

## Log Loss (Binary Cross-Entropy)

The standard loss function for logistic regression is **log loss** or **binary cross-entropy**:

$$L = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]$$

where:
- $y$ is the true label (0 or 1)
- $p$ is the predicted probability $\sigma(z)$

### Intuition

| True Label | Predicted $p$ | Loss | Interpretation |
|------------|---------------|------|----------------|
| 1 | 0.9 | $-\log(0.9) \approx 0.105$ | Confident & correct → low loss |
| 1 | 0.1 | $-\log(0.1) \approx 2.303$ | Confident & wrong → high loss |
| 0 | 0.1 | $-\log(1-0.1) \approx 0.105$ | Confident & correct → low loss |
| 0 | 0.9 | $-\log(1-0.9) \approx 2.303$ | Confident & wrong → high loss |

The logarithm penalizes confident wrong predictions heavily while giving small penalties to uncertain predictions. The cost function is designed such that if the actual label is 1 and the model predicts a probability near 0, the cost $-\log(p)$ approaches infinity. This "punishes" the model heavily for being confident and wrong.

## Detailed Loss Breakdown

### For a Positive Instance (y = 1)

We expect the model to predict a probability ≥ 0.5. The cost formula is $-\log(\hat{p})$:

| Predicted Probability ($\hat{p}$) | Accuracy Level | Cost Formula | Cost Value | Interpretation |
|---|---|---|---|---|
| 1.0 | 100% Correct | $-\log(1.0)$ | 0 | Perfect. No error, no penalty. |
| 0.7 | Mostly Correct | $-\log(0.7)$ | 0.356 | Low penalty. Model is on the right track. |
| 0.3 | Mostly Wrong | $-\log(0.3)$ | 1.204 | Higher penalty. Model is leaning toward wrong class. |
| 0.0001 | Very Wrong | $-\log(10^{-4})$ | 9.21 | Severe penalty. Model is confidently incorrect. |
| 0 | Purely Incorrect | $-\log(0)$ | ∞ | Maximum penalty. Log(0) is undefined, representing infinite cost. |

### For a Negative Instance (y = 0)

We expect the model to predict a probability < 0.5. The cost formula is $-\log(1 - \hat{p})$:

| Predicted Probability ($\hat{p}$) | Accuracy Level | Cost Formula | Cost Value | Interpretation |
|---|---|---|---|---|
| 1.0 | Purely Incorrect | $-\log(1 - 1.0)$ | ∞ | Maximum penalty. Model is 100% sure it's positive, but it's actually negative. |
| 0.7 | Mostly Wrong | $-\log(1 - 0.7)$ | 1.204 | Higher penalty. Model is leaning toward wrong class. |
| 0.3 | Mostly Correct | $-\log(1 - 0.3)$ | 0.356 | Low penalty. Model is correctly identifying it as likely negative. |
| 0.0001 | Very Correct | $-\log(1 - 10^{-4})$ | 0.0001 | Negligible penalty. Model is nearly certain and correct. |
| 0 | 100% Correct | $-\log(1 - 0)$ | 0 | Perfect. No error, no penalty. |

## Cost Function

The **cost function** for the entire dataset is the average log loss:

$$J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log\left(p^{(i)}\right) + (1 - y^{(i)}) \log\left(1 - p^{(i)}\right) \right]$$

where $m$ is the number of training examples.

> **Tip**: Log Loss value is the measure of uncertainty of our predicted labels based on how it varies from the actual label. It measures the amount of divergence of predicted probability with the actual label. So lesser the log loss value, more the perfectness of the model. For a perfect model, log loss value = 0. It is the evaluation measure to check the performance of the classification model.

## Why Log Loss Works

1. **Convex**: The log loss surface is convex with respect to the weights, guaranteeing that gradient descent will find the global minimum
2. **Probabilistically grounded**: Derived from the principle of **maximum likelihood estimation** — it finds the parameters that make the observed data most probable
3. **Appropriate penalties**: Confident wrong predictions receive exponentially larger penalties
4. **No closed-form solution**: There is no known closed-form equation (like the Normal equation) to compute the optimal weights analytically. However, because the function is convex, gradient descent is guaranteed to converge to the global minimum

## The Gradient of Log Loss

The partial derivatives of the cost function with regard to the $j$-th model parameter $\beta_j$ are:

$$\frac{\partial J}{\partial \beta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( \sigma(\boldsymbol{\beta}^T \mathbf{x}^{(i)}) - y^{(i)} \right) x_j^{(i)}$$

This computes the prediction error, multiplies it by the $j$-th feature value, and then averages over all training instances. Once you have the gradient vector containing all partial derivatives, you can use it in the batch gradient descent algorithm.

## Code Example

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

# Sample data
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# Train model
model = LogisticRegression()
model.fit(X, y)

# Get predicted probabilities
y_prob = model.predict_proba(X)
print("Predicted probabilities (class 0, class 1):")
print(y_prob)

# Compute log loss
loss = log_loss(y, y_prob)
print(f"\nLog Loss: {loss:.4f}")

# Manual computation for verification
def compute_log_loss(y_true, y_prob):
    epsilon = 1e-15  # Clip probabilities to avoid log(0)
    y_prob = np.clip(y_prob, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_prob[:, 1]) + 
                    (1 - y_true) * np.log(1 - y_prob[:, 1]))

manual_loss = compute_log_loss(y, y_prob)
print(f"Manual Log Loss: {manual_loss:.4f}")
```

## Topics to Cover

- Cross-entropy loss
- Log loss
- Binary cross-entropy

## Related Notes

- [[03. Sigmoid Function]]
- [[05. Optimization Techniques]]

## Resources

- Lectures: 21-24
- Hands-On Machine Learning (Géron), Chapter 4

---

## Summary

Log loss (binary cross-entropy) is the standard loss function for logistic regression, addressing MSE's limitations of non-convexity and inappropriate penalties. It penalizes confident wrong predictions exponentially more than uncertain ones, producing a convex cost surface that gradient descent can optimize effectively. This makes it ideal for training probabilistic classifiers.