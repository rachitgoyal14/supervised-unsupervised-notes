---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Loss Function

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## Why Not MSE?

Using **Mean Squared Error (MSE)** for logistic regression creates several problems:

### Non-Convex Loss Surface

The MSE loss function for logistic regression is **non-convex** — it has multiple local minima and maxima. When using gradient descent, the optimization algorithm can get stuck in these local minima rather than finding the global optimum.

### Inappropriate Penalty Structure

MSE penalizes confident wrong predictions proportionally to the squared error, but this doesn't align with how we want to train classifiers. A confident wrong prediction should be penalized more heavily than an uncertain one.

## Log Loss (Binary Cross-Entropy)

The standard loss function for logistic regression is **log loss** or **binary cross-entropy**:

$$L = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]$$

where:
- $y$ is the true label (0 or 1)
- $p$ is the predicted probability $\sigma(z)$

### Intuition

| True Label | Predicted $p$ | Loss | Interpretation |
|------------|---------------|------|----------------|
| 1 | 0.9 | $-\log(0.9) \approx 0.105$ | Confident & correct → low loss |
| 1 | 0.1 | $-\log(0.1) \approx 2.303$ | Confident & wrong → high loss |
| 0 | 0.1 | $-\log(1-0.1) \approx 0.105$ | Confident & correct → low loss |
| 0 | 0.9 | $-\log(1-0.9) \approx 2.303$ | Confident & wrong → high loss |

The logarithm penalizes confident wrong predictions heavily while giving small penalties to uncertain predictions.

## Cost Function

The **cost function** for the entire dataset is the average log loss:

$$J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log\left(p^{(i)}\right) + (1 - y^{(i)}) \log\left(1 - p^{(i)}\right) \right]$$

where $m$ is the number of training examples.

## Why Log Loss Works

1. **Convex**: The log loss surface is convex with respect to the weights, guaranteeing that gradient descent will find the global minimum
2. **Probabilistically grounded**: Derived from the principle of maximum likelihood estimation
3. **Appropriate penalties**: Confident wrong predictions receive exponentially larger penalties

## Code Example

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

# Sample data
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# Train model
model = LogisticRegression()
model.fit(X, y)

# Get predicted probabilities
y_prob = model.predict_proba(X)
print("Predicted probabilities (class 0, class 1):")
print(y_prob)

# Compute log loss
loss = log_loss(y, y_prob)
print(f"\nLog Loss: {loss:.4f}")

# Manual computation for verification
def compute_log_loss(y_true, y_prob):
    epsilon = 1e-15  # Clip probabilities to avoid log(0)
    y_prob = np.clip(y_prob, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_prob[:, 1]) + 
                    (1 - y_true) * np.log(1 - y_prob[:, 1]))

manual_loss = compute_log_loss(y, y_prob)
print(f"Manual Log Loss: {manual_loss:.4f}")
```

## Topics to Cover

- Cross-entropy loss
- Log loss
- Binary cross-entropy

## Related Notes

- [[03. Sigmoid Function]]
- [[05. Optimization Techniques]]

## Resources

- Lectures: 21-24
- Hands-On Machine Learning (Géron), Chapter 4

---

## Summary

Log loss (binary cross-entropy) is the standard loss function for logistic regression, addressing MSE's limitations of non-convexity and inappropriate penalties. It penalizes confident wrong predictions exponentially more than uncertain ones, producing a convex cost surface that gradient descent can optimize effectively. This makes it ideal for training probabilistic classifiers.

