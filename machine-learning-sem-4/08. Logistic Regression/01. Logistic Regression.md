---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Logistic Regression

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## What is Logistic Regression?

**Logistic regression** is a supervised classification algorithm that estimates the probability that a given input belongs to a particular class. Although the name contains "regression," it is fundamentally a **classification algorithm** — it predicts discrete class labels (0 or 1) rather than continuous values.

The algorithm measures the relationship between one or more independent variables (features X) and a categorical dependent variable (Y) by estimating probabilities using a logistic (sigmoid) function. The term "regression" persists because the method estimates the probability of class membership, similar to how linear regression estimates continuous values.

> **Key Requirement**: Data should be **linearly separable** for logistic regression to work effectively.

### Real-World Examples

1. **Medical diagnosis**: Predicting whether a patient has a disease (yes/no) based on symptoms and test results
2. **Credit scoring**: Determining whether a loan applicant will default (yes/no) based on financial history
3. **Email spam classification**: Classifying emails as spam or not spam based on content features
4. **Student placement prediction**: Predicting whether a student will get placed based on CGPA and other factors
5. **Banking**: Loan Default (1) vs. Loan Repaid (0)
6. **Medicine**: Tumor Malignant (1) vs. Benign (0)
7. **Manufacturing**: Part Defective (1) vs. Functional (0)
8. **E-commerce**: User will Buy (1) vs. Will Not Buy (0)

## Binary vs. Multiclass Classification

- **Binary Classification**: The output has only two possible classes (e.g., Yes/No, Spam/Not Spam, 1/0)
- **Multiclass Classification**: The output can be one of many classes (e.g., classifying a fruit image as Apple, Banana, or Orange)

Logistic regression is natively a binary classifier, but can be extended to multiclass problems using strategies like One-vs-Rest (OvR) or multinomial logistic regression (Softmax).

## Why Not Linear Regression?

Linear regression fails for classification problems for several key reasons:

### 1. Nature of the Problem

Linear regression is designed for **continuous** output values, while classification deals with **discrete** class labels. When trained on binary data (0 or 1), linear regression tries to fit a straight line that minimizes squared error — it doesn't inherently understand that it's working with class probabilities.

In regression the target variable is of continuous type, whereas in classification the target variable is of discrete type. Linear regression predicts a value `y`. If we train it on class data, it might output `y = 0.7`. We can interpret this as a probability, but linear regression doesn't "know" it's dealing with classes — it tries to fit a line to minimize error, not separate groups.

### 2. The Outlier Problem

Linear regression uses **Mean Squared Error (MSE)**, which heavily penalizes large errors. Large errors get squared → very high penalty. If one extreme point exists, the decision boundary shifts badly. A single outlier can create a large residual, causing the regression line to shift dramatically and distort the entire decision boundary. This makes the model extremely sensitive to misclassified points and is dangerous for classification tasks.

### 3. Invalid Probability Range

The output of linear regression ranges from **−∞ to +∞**, but probabilities must lie between **0 and 1**. Linear regression can easily predict values like −2, 1.8, or 5.3 — none of which are valid probabilities. Without proper bounds, interpreting outputs as confidence scores is meaningless.

### 4. Prediction Changes Suddenly

Linear regression assumes a constant rate of change. In classification, changes are often abrupt (e.g., a "tipping point" where a student passes or fails). A straight line cannot model this sharp transition.

## How Logistic Regression Solves This

Logistic regression addresses these problems by:

1. **Squashing outputs** to the [0, 1] range using the **sigmoid function** (covered in [[03. Sigmoid Function]])
2. **Using a different loss function** (log loss / binary cross-entropy) that properly penalizes classification errors
3. **Producing probabilistic outputs** that can be directly interpreted as confidence levels

## How Logistic Regression Works

Just like a linear regression model, a logistic regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly, it outputs the **logistic (sigmoid)** of this result:

$$\hat{p} = \sigma(\boldsymbol{\beta}^T \mathbf{x}) = \frac{1}{1 + e^{-\boldsymbol{\beta}^T \mathbf{x}}}$$

Once the model has estimated the probability $\hat{p}$, it makes its prediction:

$$\hat{y} = \begin{cases} 1 & \text{if } \hat{p} \geq 0.5 \\ 0 & \text{if } \hat{p} < 0.5 \end{cases}$$

A logistic regression model using the default threshold of 50% probability predicts 1 if $\boldsymbol{\beta}^T \mathbf{x}$ is positive and 0 if it is negative.

## Training Objective

The objective of training is to set the parameter vector $\boldsymbol{\beta}$ so that the model estimates **high probabilities for positive instances** (y = 1) and **low probabilities for negative instances** (y = 0).

The cost function over the whole training set is the average cost over all training instances, known as the **log loss**:

$$J(\boldsymbol{\beta}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log\left(\hat{p}^{(i)}\right) + (1 - y^{(i)}) \log\left(1 - \hat{p}^{(i)}\right) \right]$$

> **Tip**: Log Loss value is the measure of uncertainty of our predicted labels based on how it varies from the actual label. It measures the amount of divergence of predicted probability with the actual label. So lesser the log loss value, more the perfectness of the model. For a perfect model, log loss value = 0.

There is **no known closed-form equation** to compute the value of $\boldsymbol{\beta}$ that minimizes this cost function (there is no equivalent of the Normal equation). However, this cost function is **convex**, so gradient descent (or any other optimization algorithm) is guaranteed to find the global minimum.

## scikit-learn Implementation

### Core Methods

The `sklearn.linear_model.LogisticRegression` class provides the following important methods:

- **`fit(X, y)`**: Trains the model. Fits the model according to the given training data X and target labels y. The solver optimizes the cost function (log-loss + regularization penalty) to find the best coefficients `coef_` and intercept `intercept_`.
- **`predict(X)`**: Predicts the class labels for the provided data samples. Returns an array of shape `(n_samples,)` containing the predicted class labels.
- **`predict_proba(X)`**: Returns probability estimates for all possible classes. Returns an array of shape `(n_samples, n_classes)`. Each row represents a sample, and each column represents the probability of that sample belonging to a specific class. The sum of probabilities in each row is always 1.
- **`predict_log_proba(X)`**: Returns the logarithm of the probability estimates (`log(predict_proba(X))`). Log-probabilities improve numerical stability and avoid "underflow" issues where very small probabilities are rounded to zero.

### Code Example

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# Sample data: CGPA and placement outcome
X = np.array([
    [8.1], [7.5], [6.2], [9.0], [5.5], 
    [8.5], [6.8], [7.2], [4.5], [8.8]
])
y = np.array([1, 1, 0, 1, 0, 1, 1, 1, 0, 1])

# Fit logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Predict probabilities
prob = model.predict_proba([[7.0]])
print(f"P(not placed): {prob[0][0]:.3f}")
print(f"P(placed): {prob[0][1]:.3f}")

# Predict class label
label = model.predict([[7.0]])
print(f"Predicted class: {label[0]}")
```

### Activity: Logistic Regression on Iris Dataset

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Binary classification: class 0 vs rest
y_binary = (y == 0).astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_binary, test_size=0.2, random_state=42
)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))
```

## Topics to Cover

- Binary classification
- Logistic function
- Decision boundary

## Related Notes

- [[03. Sigmoid Function]]
- [[04. Loss Function]]

## Resources

- Lectures: 21-24
- Hands-On Machine Learning (Géron), Chapter 4

---

## Summary

Logistic regression is a classification algorithm that estimates class membership probabilities by applying a sigmoid function to a linear combination of input features. It solves the key limitations of linear regression for classification — invalid probability ranges, outlier sensitivity, and inappropriate loss functions — making it a foundational tool for binary classification tasks. The model trains by minimizing log loss (binary cross-entropy), a convex function that gradient descent can reliably optimize to find the global minimum.