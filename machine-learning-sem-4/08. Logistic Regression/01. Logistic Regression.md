---
tags: [topic, status/placeholder]
lecture: [21]
created: 2026-02-13
---

# Logistic Regression

## Overview

> This note is a placeholder for future content based on Lecture 21-24.

## What is Logistic Regression?

**Logistic regression** is a supervised classification algorithm that estimates the probability that a given input belongs to a particular class. Although the name contains "regression," it is fundamentally a **classification algorithm** — it predicts discrete class labels (0 or 1) rather than continuous values.

The algorithm measures the relationship between one or more independent variables (features X) and a categorical dependent variable (Y) by estimating probabilities using a logistic (sigmoid) function. The term "regression" persists because the method estimates the probability of class membership, similar to how linear regression estimates continuous values.

### Real-World Examples

1. **Medical diagnosis**: Predicting whether a patient has a disease (yes/no) based on symptoms and test results
2. **Credit scoring**: Determining whether a loan applicant will default (yes/no) based on financial history
3. **Email spam classification**: Classifying emails as spam or not spam based on content features
4. **Student placement prediction**: Predicting whether a student will get placed based on CGPA and other factors

## Why Not Linear Regression?

Linear regression fails for classification problems for three key reasons:

### 1. Nature of the Problem

Linear regression is designed for **continuous** output values, while classification deals with **discrete** class labels. When trained on binary data (0 or 1), linear regression tries to fit a straight line that minimizes squared error — it doesn't inherently understand that it's working with class probabilities.

### 2. The Outlier Problem

Linear regression uses **Mean Squared Error (MSE)**, which heavily penalizes large errors. A single outlier can create a large residual, causing the regression line to shift dramatically and distort the entire decision boundary. This makes the model extremely sensitive to misclassified points.

### 3. Invalid Probability Range

The output of linear regression ranges from **−∞ to +∞**, but probabilities must lie between **0 and 1**. Linear regression can easily predict values like −2, 1.8, or 5.3 — none of which are valid probabilities. Without proper bounds, interpreting outputs as confidence scores is meaningless.

## How Logistic Regression Solves This

Logistic regression addresses these problems by:

1. **Squashing outputs** to the [0, 1] range using the **sigmoid function** (covered in [[03. Sigmoid Function]])
2. **Using a different loss function** (log loss / binary cross-entropy) that properly penalizes classification errors
3. **Producing probabilistic outputs** that can be directly interpreted as confidence levels

## scikit-learn Implementation

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# Sample data: CGPA and placement outcome
X = np.array([
    [8.1], [7.5], [6.2], [9.0], [5.5], 
    [8.5], [6.8], [7.2], [4.5], [8.8]
])
y = np.array([1, 1, 0, 1, 0, 1, 1, 1, 0, 1])

# Fit logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Predict probabilities
prob = model.predict_proba([[7.0]])
print(f"P(not placed): {prob[0][0]:.3f}")
print(f"P(placed): {prob[0][1]:.3f}")
```

## Topics to Cover

- Binary classification
- Logistic function
- Decision boundary

## Related Notes

- [[03. Sigmoid Function]]
- [[04. Loss Function]]

## Resources

- Lectures: 21-24
- Hands-On Machine Learning (Géron), Chapter 4

---

## Summary

Logistic regression is a classification algorithm that estimates class membership probabilities by applying a sigmoid function to a linear combination of input features. It solves the key limitations of linear regression for classification — invalid probability ranges, outlier sensitivity, and inappropriate loss functions — making it a foundational tool for binary classification tasks.

