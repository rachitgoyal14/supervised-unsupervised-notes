---
tags: [topic]
lecture: [1]
created: 2026-02-13
---

## Overfitting

**Definition:** Model memorizes data, performs well on training data but poorly on test data.

**Analogy:** Like a student who memorizes exam questions and answers without understanding concepts. They ace practice tests but fail when questions are slightly different on the real exam.

---

## Underfitting

**Definition:** Model is too simple, performs poorly on training data.

**Analogy:** Like using a straight ruler to trace a curvy mountain path. The tool is too simple to capture the complexity of the terrain.

---

## Dimensionality

**Definition:** Number of features in the data.

**Analogy:** Think of describing a house. Each feature (bedrooms, bathrooms, square footage, location, age) is a dimension. More dimensions mean more ways to describe the house.

---

## Anomaly

**Definition:** Rare or unusual data point.

**Analogy:** Like finding a penguin in a desert. It's extremely rare and doesn't fit the normal pattern of what you'd expect to see there.

---

## Novelty

**Definition:** New unseen pattern not present during training.

**Analogy:** Like a chef who learned to cook Italian and French cuisine, then encounters Thai food for the first time. It's a completely new pattern they haven't experienced before.

---

## Data Pipeline

**Definition:** Flow of data from source to model.

**Analogy:** Like a factory assembly line where raw materials enter, go through processing stations, and emerge as finished products.

---

## Upstream

**Definition:** Data source or earlier stages in the pipeline.

**Analogy:** Like the beginning of a river where water originates from mountains and springs before flowing downstream.

---

## Downstream

**Definition:** Model output or later stages in the pipeline.

**Analogy:** Like the mouth of a river where all the water eventually flows out into the ocean.

---

## Model Parameter

**Definition:** Values learned by the model during training.

**Analogy:** Like a musician's muscle memory developed through practice. The musician doesn't consciously decide these movementsâ€”they're learned automatically through repetition.

## Summary

These fundamental ML terms form the vocabulary needed to understand machine learning concepts. Key distinctions include: parameters are learned from data (like coefficients in linear regression), while hyperparameters are set before training (like learning rate); overfitting occurs when models memorize training data rather than learning generalizable patterns; underfitting happens when models are too simple to capture underlying relationships. Understanding these concepts is essential before diving into algorithms.

## Interview Questions

**Q: What's the difference between a parameter and a hyperparameter?**
Parameters are learned automatically during training (e.g., weights in a neural network, coefficients in linear regression). Hyperparameters are set by the practitioner before training begins (e.g., learning rate, number of trees in a forest, degree of polynomial).

**Q: Why is it important to have a separate test set?**
The test set represents unseen data that the model will encounter in production. Evaluating only on training data would give an overly optimistic estimate of performance because the model has already seen those examples. The test set measures generalization ability.

**Q: How do you know if your model is overfitting?**
If training error is low but test/validation error is high, the model is overfitting. You can also observe this in learning curves where training and validation loss diverge significantly.
