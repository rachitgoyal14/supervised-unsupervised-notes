---
tags: [topic]
lecture: [13]
created: 2026-02-13
---

# Batch Gradient Descent

## Overview

**Batch Gradient Descent** (also called **Vanilla Gradient Descent**) is the most straightforward variant of gradient descent optimization. In this approach, we calculate the gradient using the **entire dataset** before making a single update to the model parameters. Each iteration processes all training examples to compute the exact gradient.

This "full-batch" approach provides stable, accurate gradient estimates but can be computationally expensive for large datasets. Understanding batch GD is fundamental before learning about stochastic and mini-batch variants.

## How Batch Gradient Descent Works

The algorithm processes all $m$ training examples before updating parameters:

```
For each iteration:
    1. Compute predictions for ALL m samples: ŷ = Xβ
    2. Calculate cost using ALL samples: J(β) = MSE
    3. Compute gradient using ALL samples: ∇J = (1/m) Xᵀ(Xβ - y)
    4. Update parameters: β = β - α∇J
```

### The Update Rule

For linear regression with MSE loss:

$$\beta_{new} = \beta_{old} - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m}(h_\beta(x^{(i)}) - y^{(i)})x^{(i)}$$

Where:
- $\alpha$ = learning rate
- $m$ = number of training examples
- $h_\beta(x)$ = model prediction

## Mathematical Intuition

The gradient $\nabla J$ points in the direction of steepest ascent. By subtracting it (multiplying by negative), we move in the direction of steepest descent—the direction that decreases the cost function fastest.

The gradient is calculated as:

$$\nabla J(\beta) = \frac{2}{m}X^T(X\beta - y)$$

This uses the **entire** training set to compute one gradient.

## Convergence Behavior

Batch GD has a **smooth, monotonic** convergence:

```
Cost
  │
  │      ╱─────╲
  │    ╱         ╲
  │  ╱             ╲
  │╱                 ╲
  └───────────────────→ Iterations
     Steady descent
     to global minimum
```

Each iteration strictly decreases (or maintains) the cost—no oscillation.

## Advantages

### 1. Stable Convergence

Because the gradient is computed from all data, each step is in the "true" downhill direction. No noise or randomness.

### 2. Guaranteed Global Minimum

For convex problems (like linear regression), batch GD is guaranteed to find the global minimum because there's only one minimum to find.

### 3. Simple Implementation

The algorithm is straightforward to understand and implement—no sampling logic or special handling.

### 4. Good for Small Datasets

When your dataset fits in memory, batch GD is often the fastest approach.

## Disadvantages

### 1. Slow for Large Datasets

Must process all $m$ examples before one update. For millions of samples, this is prohibitively slow.

**Example:**
- 1 million samples: 1 update per million predictions
- SGD: 1 million updates per million predictions

### 2. Memory Intensive

All training data must fit in memory. Can't handle datasets larger than available RAM.

### 3. Can't Escape Local Minima (for non-convex)

While linear regression is convex, for neural networks, batch GD can get stuck in local minima.

### 4. No Online Learning

Can't update model with new data without retraining on entire dataset.

## When to Use Batch Gradient Descent

**Best for:**
- Small datasets (< 10,000 samples)
- When precision is critical
- Convex optimization problems
- When you have sufficient memory

**Avoid when:**
- Dataset is very large
- Data doesn't fit in memory
- Online/incremental learning needed
- Fast prototyping

## Python Implementation

```python
import numpy as np

def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    """
    Batch Gradient Descent for Linear Regression
    
    Parameters:
    -----------
    X : ndarray, shape (m, n)
        Feature matrix (m samples, n features)
    y : ndarray, shape (m,)
        Target values
    learning_rate : float
        Step size (alpha)
    n_iterations : int
        Number of iterations
        
    Returns:
    --------
    beta : ndarray
        Learned parameters
    """
    m, n = X.shape
    
    # Initialize parameters to zeros
    beta = np.zeros(n)
    
    for i in range(n_iterations):
        # 1. Compute predictions: h(x) = Xβ
        predictions = X @ beta
        
        # 2. Compute errors
        errors = predictions - y
        
        # 3. Compute gradient using ALL samples
        gradient = (1/m) * (X.T @ errors)
        
        # 4. Update parameters
        beta = beta - learning_rate * gradient
        
        # Optional: print progress
        if i % 100 == 0:
            cost = (1/(2*m)) * np.sum(errors**2)
            print(f"Iteration {i}: Cost = {cost:.4f}")
    
    return beta

# Example usage
X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])
y = np.array([2, 4, 6, 8])

beta = batch_gradient_descent(X, y, learning_rate=0.1, n_iterations=1000)
print(f"Learned parameters: {beta}")
```

## Learning Rate Considerations

The learning rate $\alpha$ significantly affects convergence:

| Learning Rate | Behavior |
|--------------|----------|
| Too small | Very slow convergence |
| Just right | Smooth, fast convergence |
| Too large | Oscillation, may diverge |
| Way too large | Diverges immediately |

### Finding the Right Learning Rate

```python
# Common values to try
learning_rates = [0.001, 0.01, 0.1, 1.0]

for lr in learning_rates:
    beta = batch_gradient_descent(X, y, learning_rate=lr, n_iterations=100)
    print(f"LR={lr}: Final cost = {cost:.4f}")
```

## Comparison with Other Variants

| Aspect | Batch GD | Stochastic GD | Mini-Batch GD |
|--------|----------|---------------|---------------|
| **Samples per update** | All (m) | 1 | b (32-256) |
| **Speed** | Slowest | Fastest | Fast |
| **Noise** | None | High | Moderate |
| **Memory** | High | Low | Medium |
| **Convergence** | Smooth | Noisy | Moderate |

## Interview Questions

**Q: Why is batch gradient descent slow for large datasets?**
Batch GD processes ALL training examples before making ONE parameter update. If you have a million samples, you make a million predictions before updating once. This is computationally expensive and doesn't scale.

**Q: Can batch gradient descent be parallelized?**
Yes, gradient computation can be parallelized since each sample's contribution to the gradient is independent. However, the entire dataset still needs to be processed before updates.

**Q: What's the main advantage of batch gradient descent over stochastic?**
Stability. The gradient computed from the full dataset is the true gradient (direction of steepest descent), not an approximation. This leads to smooth, monotonic convergence.

## Summary

Batch Gradient Descent computes gradients using the entire dataset before each update:
- **Pros**: Stable, smooth convergence; guaranteed for convex problems
- **Cons**: Slow for large datasets; high memory requirements
- **Best for**: Small datasets where precision matters

The next step is learning about **Stochastic Gradient Descent** (SGD), which processes one sample at a time for much faster iteration.

## Related Notes

- [[03. Gradient Descent Introduction]]
- [[05. Stochastic Gradient Descent]]
- [[06. Mini-Batch Gradient Descent]]

## Resources

- Lectures: 13-14
- Book: "Hands-On Machine Learning" Chapter 4
