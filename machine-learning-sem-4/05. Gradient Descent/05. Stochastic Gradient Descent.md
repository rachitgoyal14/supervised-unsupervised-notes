---
tags: [topic]
lecture: [13]
created: 2026-02-13
---

# Stochastic Gradient Descent

## Overview

**Stochastic Gradient Descent (SGD)** takes a radically different approach from batch GD: instead of computing the gradient using all training samples, SGD computes the gradient and updates parameters using just **one random training sample** at a time. This makes SGD incredibly fast and scalable, capable of handling millions of samples and even streaming data.

The "stochastic" part comes from the randomness—since we use only one sample, the gradient is a noisy estimate of the true gradient. This noise, while seemingly problematic, actually helps SGD escape shallow local minima in non-convex problems.

## How Stochastic Gradient Descent Works

The algorithm processes one sample at a time:

```
For each epoch (pass through data):
    Shuffle the training data
    For each sample i:
        1. Compute prediction: ŷᵢ = h_β(xᵢ)
        2. Compute error: error = ŷᵢ - yᵢ
        3. Compute gradient for sample: ∇Jᵢ = error × xᵢ
        4. Update: β = β - α × ∇Jᵢ
```

### The Update Rule (Per Sample)

For a single sample $(x^{(i)}, y^{(i)})$:

$$\beta = \beta - \alpha \cdot (h_\beta(x^{(i)}) - y^{(i)})x^{(i)}$$

This is the same as batch GD, but with $m=1$.

## Convergence Behavior

SGD has a **noisy, oscillating** convergence:

```
Cost
  │
  │   ↗ ↘  ↗  ↘  ↗
  │  ↗  ↘ ↗   ↘ ↗  ↘
  │ ↗    ↘  ↗   ↘  ↗  ↘
  │╱         ↗      ↘
  └──────────────────→ Iterations
     Fluctuates but
     generally decreases
```

The noise is due to using an imperfect gradient estimate.

## Advantages

### 1. Extremely Fast

Each update is computationally cheap—we process ONE sample, make ONE prediction, compute ONE gradient, and update. We make $m$ updates per epoch instead of 1.

**Comparison:**
- Batch GD: 1 update per epoch (m predictions)
- SGD: m updates per epoch (m predictions)

Both do the same total work, but SGD gives $m\times$ more parameter updates!

### 2. Can Escape Local Minima

The noise in gradient estimates helps SGD jump out of shallow local minima:

```
     Loss
      │
      │  local   global
      │   min     min
      │    ↘     ↙
      │     ↘   ↙
      │      ↘ ↙
      │       ↙
      │      ↗↘
      │    ↗  ↘
      │   ↗    ↘
      └──────────→ Parameters
      
    SGD noise can "jump" out
    of local minima
```

### 3. Online Learning

SGD naturally supports incremental learning—update the model as new data arrives:

```python
for new_sample in streaming_data():
    # Update immediately with new data
    beta = beta - alpha * (predict(new_sample) - actual) * features
```

### 4. Memory Efficient

Only one sample needs to be in memory at a time. Can train on datasets larger than RAM.

## Disadvantages

### 1. Noisy Convergence

The gradient is approximate, so:
- Convergence is not smooth
- May never exactly reach minimum
- Oscillates around the optimum

### 2. Requires Learning Rate Scheduling

The noise can cause instability. Typically, we reduce learning rate over time:

$$\alpha_t = \frac{\alpha_0}{1 + decay \times t}$$

### 3. More Iterations Needed

While each iteration is faster, you often need more iterations to converge.

## Learning Rate Scheduling

To handle the noise, common strategies:

### 1. Constant Learning Rate
```python
alpha = 0.01  # Fixed throughout
```

### 2. Time-Based Decay
```python
alpha = alpha_0 / (1 + decay * epoch)
```

### 3. Step Decay
```python
if epoch % 10 == 0:
    alpha = alpha * 0.5
```

### 4. Exponential Decay
```python
alpha = alpha_0 * exp(-decay * epoch)
```

## When to Use SGD

**Best for:**
- Large datasets (> 100,000 samples)
- Online/incremental learning
- Non-convex problems (neural networks)
- When getting "close enough" is acceptable

**Avoid when:**
- Need exact solution for convex problems
- Small datasets (batch GD is faster)
- Stable convergence is critical

## Python Implementation

```python
import numpy as np

def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=100, decay=0.01):
    """
    Stochastic Gradient Descent for Linear Regression
    
    Parameters:
    -----------
    X : ndarray, shape (m, n)
        Feature matrix
    y : ndarray, shape (m,)
        Target values
    learning_rate : float
        Initial learning rate
    n_epochs : int
        Number of passes through data
    decay : float
        Learning rate decay
    """
    m, n = X.shape
    beta = np.zeros(n)
    
    for epoch in range(n_epochs):
        # Decay learning rate
        alpha = learning_rate / (1 + decay * epoch)
        
        # Shuffle data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        total_error = 0
        for i in range(m):
            # Single sample
            xi = X_shuffled[i:i+1]  # Keep 2D
            yi = y_shuffled[i]
            
            # Prediction and error
            prediction = xi @ beta
            error = prediction - yi
            
            # Gradient for this sample
            gradient = error * xi.flatten()
            
            # Update
            beta = beta - alpha * gradient
            total_error += error ** 2
        
        if epoch % 10 == 0:
            cost = total_error / (2 * m)
            print(f"Epoch {epoch}: Cost = {cost:.4f}, LR = {alpha:.4f}")
    
    return beta

# Example
X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
y = np.array([2, 4, 6, 8, 10])

beta = stochastic_gradient_descent(X, y, learning_rate=0.1, n_epochs=100)
print(f"Parameters: {beta}")
```

## Scikit-Learn Implementation

```python
from sklearn.linear_model import SGDRegressor

model = SGDRegressor(
    loss='squared_error',
    learning_rate='invscaling',  # Decreases over time
    eta0=0.01,
    power_t=0.25,
    max_iter=1000,
    random_state=42
)

model.fit(X, y)
print(f"Parameters: {model.coef_}")
```

## Interview Questions

**Q: Why does SGD work well for large datasets?**
SGD processes one sample at a time, making O(1) updates rather than O(m). This means total computation is O(m × iterations) vs O(m × iterations) for batch, but SGD gives m times more parameter updates for the same work. For large m, this extra updating quickly leads to good solutions.

**Q: Why might SGD converge to a better solution than batch GD for neural networks?**
The noise in SGD's gradient estimates acts as a form of regularization. This noise helps the optimization escape sharp local minima that generalize poorly, leading to solutions that perform better on test data.

**Q: What is the difference between SGD and batch GD in terms of gradient computation?**
Batch GD computes the exact gradient: $\nabla J = \frac{1}{m}\sum_{i=1}^{m}\nabla J_i$. SGD approximates this with a single sample: $\nabla J \approx \nabla J_i$. The approximation is noisy but much faster to compute.

## Summary

Stochastic Gradient Descent updates parameters using one sample at a time:
- **Pros**: Fast updates, can escape local minima, supports online learning
- **Cons**: Noisy convergence, requires learning rate scheduling
- **Best for**: Large datasets and non-convex problems (neural networks)

**Key insight**: The noise in SGD is actually helpful—it prevents getting stuck in poor local minima and often leads to better generalization.

## Related Notes

- [[03. Gradient Descent Introduction]]
- [[04. Batch Gradient Descent]]
- [[06. Mini-Batch Gradient Descent]]

## Resources

- Lectures: 13-14
- Book: "Hands-On Machine Learning" Chapter 4
