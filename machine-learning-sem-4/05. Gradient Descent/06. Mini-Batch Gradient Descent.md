---
tags: [topic]
lecture: [13]
created: 2026-02-13
---

# Mini-Batch Gradient Descent

## Overview

**Mini-Batch Gradient Descent** strikes a balance between batch GD (using all data) and stochastic GD (using one sample). In mini-batch GD, we use a small **batch** of samples (typically 32, 64, or 128) to compute the gradient before each update. This combines the stability of batch GD with the speed of SGD, making it the most commonly used gradient descent variant in practice.

In deep learning specifically, "gradient descent" almost always refers to mini-batch GD. It's the workhorse behind most neural network training.

## How Mini-Batch Gradient Descent Works

```
For each epoch:
    Shuffle the data
    Divide into batches of size b
    For each batch b:
        1. Compute predictions for batch: ŷ_batch = X_batch @ β
        2. Compute gradient: ∇J = (1/b) X_batchᵀ(ŷ_batch - y_batch)
        3. Update: β = β - α × ∇J
```

### The Update Rule (Per Batch)

$$\beta = \beta - \alpha \cdot \frac{1}{b}\sum_{i=1}^{b}(h_\beta(x^{(i)}) - y^{(i)})x^{(i)}$$

Where $b$ is the batch size (e.g., 32, 64, 128).

## Convergence Behavior

Mini-batch GD has **moderate noise**—smoother than SGD but not perfectly smooth like batch GD:

```
Cost
  │
  │    ↗ ↘  ↗↘   ↗
  │   ↗  ↘ ↗ ↘ ↗  ↘
  │  ↗    ↘    ↗  ↘
  │ ↗          ↗↘
  │╱               ↘
  └──────────────────→ Iterations
    
    Moderate oscillation,
    steady progress
```

## Why Mini-Batch Works So Well

### 1. Computational Efficiency

Modern CPUs and GPUs are optimized for matrix operations. Processing a batch allows vectorized computation—much faster than processing samples one at a time.

```python
# Slow: Loop over samples
for i in range(1000):
    error = prediction[i] - y[i]
    gradient += error * X[i]

# Fast: Vectorized batch operation
errors = predictions - y
gradient = X.T @ errors  # Single matrix operation!
```

### 2. Better Gradient Estimates

A batch provides a better gradient estimate than a single sample (SGD) while being faster than all samples (batch GD).

| Batch Size | Gradient Quality | Speed |
|------------|-----------------|-------|
| 1 (SGD) | Noisy | Fastest |
| 32-256 | Good balance | Fast |
| All (Batch) | Perfect | Slow |

### 3. Escape Poor Local Minima

Like SGD, the noise from small batches helps escape shallow local minima in non-convex problems.

### 4. Regularization Effect

Small batches provide a form of implicit regularization. The noise in gradient estimates acts like added noise that can improve generalization.

## Choosing Batch Size

Common batch sizes and trade-offs:

| Batch Size | Pros | Cons |
|------------|------|------|
| 16-32 | Better generalization, more regularization | More updates needed |
| 64-128 | Good balance | Default choice |
| 256-512 | Faster training, stable | May need more epochs |
| 2048+ | Very fast, stable | Less regularization, more memory |

**Rule of thumb**: Start with 64 or 128 as default.

### GPU Considerations

GPUs are most efficient with batch sizes that are:
- Powers of 2 (32, 64, 128, 256, 512)
- Multiple of 32 (NVIDIA uses 32 "warps")

## When to Use Mini-Batch GD

**Best for:**
- Almost all practical applications
- Training neural networks
- Large datasets
- When you want balance of speed and stability

**It's the default choice** for most ML problems.

## Python Implementation

```python
import numpy as np

def mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, n_epochs=100):
    """
    Mini-Batch Gradient Descent for Linear Regression
    
    Parameters:
    -----------
    X : ndarray, shape (m, n)
        Feature matrix
    y : ndarray, shape (m,)
        Target values
    batch_size : int
        Number of samples per batch
    learning_rate : float
        Step size
    n_epochs : int
        Number of passes through data
    """
    m, n = X.shape
    beta = np.zeros(n)
    
    for epoch in range(n_epochs):
        # Shuffle data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # Process in batches
        for start in range(0, m, batch_size):
            end = min(start + batch_size, m)
            
            X_batch = X_shuffled[start:end]
            y_batch = y_shuffled[start:end]
            
            # Vectorized gradient computation
            predictions = X_batch @ beta
            errors = predictions - y_batch
            gradient = (1/len(errors)) * (X_batch.T @ errors)
            
            # Update
            beta = beta - learning_rate * gradient
        
        if epoch % 10 == 0:
            cost = np.mean((X @ beta - y) ** 2) / 2
            print(f"Epoch {epoch}: Cost = {cost:.4f}")
    
    return beta

# Example
X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8]])
y = np.array([2, 4, 6, 8, 10, 12, 14, 16])

beta = mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.1, n_epochs=100)
print(f"Parameters: {beta}")
```

## Scikit-Learn Implementation

```python
from sklearn.linear_model import SGDRegressor

# Mini-batch is the default for SGDRegressor
model = SGDRegressor(
    loss='squared_error',
    learning_rate='constant',
    eta0=0.01,
    max_iter=1000,
    random_state=42
)

# Set batch size via partial_fit (online learning)
model.partial_fit(X_batch, y_batch)

# Or fit with specific batch (treats full data as one batch)
model.fit(X, y)
```

## Comparison Summary

| Aspect | Batch GD | Mini-Batch GD | SGD |
|--------|----------|---------------|-----|
| **Batch Size** | All (m) | b (32-256) | 1 |
| **Gradient** | Exact | Approximate | Very noisy |
| **Speed** | Slowest | Fast | Fastest |
| **Memory** | High | Medium | Low |
| **Convergence** | Smooth | Moderate | Noisy |
| **Generalization** | Can overfit | Good | Good |
| **Use Case** | Small data | Most cases | Large data |

## Advanced Tips

### Learning Rate Scheduling

```python
# Use learning rate scheduler
scheduler = StepLR(optimizer, step_size=10, gamma=0.5)

for epoch in range(100):
    train()
    scheduler.step()  # Reduce LR
```

### Momentum

Add momentum to reduce oscillation:

```python
# Momentum update
velocity = momentum * velocity - learning_rate * gradient
beta = beta + velocity
```

### Adaptive Methods

Consider Adam, RMSprop, or Adagrad—these automatically adjust learning rates.

## Interview Questions

**Q: Why is mini-batch GD the most commonly used variant?**
Mini-batch combines the best of both worlds: computational efficiency through vectorization, better gradient estimates than SGD, and some noise that helps generalization. It works well for datasets of any size and is the standard for neural network training.

**Q: What happens if batch size is too small?**
Very small batches (e.g., 1-4) create very noisy gradients that can cause unstable convergence. You might need significantly more epochs to converge, and the noise might prevent reaching a good minimum.

**Q: What happens if batch size is too large?**
Large batches lose the "regularization" effect of noise, potentially leading to worse generalization. They also require more memory and may be slower per epoch (less frequent updates).

**Q: How do you choose batch size?**
Start with 64 or 128 as a default. Consider:
- GPU memory (larger batches need more memory)
- Dataset size (smaller datasets may need smaller batches)
- Experiment (try 32, 64, 128, 256)

## Summary

Mini-Batch Gradient Descent uses small batches (typically 32-256) to balance speed and stability:
- **Pros**: Fast, good gradient estimates, works well on GPUs, provides regularization
- **Cons**: Requires tuning batch size, some noise in convergence
- **Best for**: Almost all practical applications (the default choice)

This is the workhorse of modern machine learning—understanding it is essential for any ML practitioner.

## Related Notes

- [[03. Gradient Descent Introduction]]
- [[04. Batch Gradient Descent]]
- [[05. Stochastic Gradient Descent]]

## Resources

- Lectures: 13-14
- Book: "Hands-On Machine Learning" Chapter 4
