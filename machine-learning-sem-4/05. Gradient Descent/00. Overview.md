# Gradient Descent - Overview

## ğŸ“š Contents

1. [[01. Model Parameters|Model Parameters]]
2. [[02. Closed Form vs Gradient Descent|Closed Form vs Gradient Descent]]
3. [[03. Gradient Descent Introduction|Gradient Descent Introduction]]
4. [[04. Batch Gradient Descent|Batch Gradient Descent]]
5. [[05. Stochastic Gradient Descent|Stochastic Gradient Descent]]
6. [[06. Mini-Batch Gradient Descent|Mini-Batch Gradient Descent]]

## ğŸ¯ Learning Objectives

- Understand model parameters
- Learn gradient descent optimization
- Compare closed-form vs iterative solutions

## ğŸ”— Related Sections

- [[04. Loss Functions/00. Overview|Loss Functions]]
- [[06. Advanced Regression/00. Overview|Advanced Regression]]

## ğŸ“ Resources

- Lectures: 13-14
- Notebooks: `../../gradient-descent/`

Gradient descent is the optimization algorithm that powers most machine learning. This section covers how gradient descent iteratively adjusts model parameters to minimize loss, compares the three variants (batch, stochastic, and mini-batch), and explains when to use closed-form solutions versus iterative optimization. Understanding gradient descent is essential for mastering both classical algorithms and deep learning.
