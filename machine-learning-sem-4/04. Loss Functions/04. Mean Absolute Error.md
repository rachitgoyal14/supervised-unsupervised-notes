---
tags: [topic]
lecture: [10]
created: 2026-02-13
---

# Mean Absolute Error

## Definition

Mean Absolute Error (MAE) measures the average magnitude of errors in a set of predictions, without considering their direction. It's the mean of the absolute differences between predicted and actual values.

## Mathematical Formula

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

Where:
- $n$ = number of data points
- $y_i$ = actual value
- $\hat{y}_i$ = predicted value
- $|...|$ = absolute value

## Why Absolute Value?

Taking the absolute value ensures all errors are positiveâ€”preventing the cancellation that occurs with raw error sums:

- Error of +5 and -5 don't cancel out
- Instead: |+5| + |-5| = 5 + 5 = 10

This gives us the true average magnitude of errors.

## Calculation Example

| Actual (y) | Predicted (Å·) | Error (y - Å·) | Absolute Error |
|------------|---------------|---------------|---------------|
| 100        | 110           | -10           | 10            |
| 200        | 190           | +10           | 10            |
| 150        | 155           | -5            | 5             |
| 80         | 85            | -5            | 5             |
| 300        | 295           | +5            | 5             |

$$MAE = (10 + 10 + 5 + 5 + 5) / 5 = 35 / 5 = 7$$

**Interpretation**: On average, predictions are off by 7 units.

## Advantages of MAE

### 1. Robust to Outliers

MAE treats all errors equally, unlike MSE which heavily penalizes large errors.

**Example:**
- Dataset: [1, 2, 3, 4, 100]
- Prediction: [2, 3, 4, 5, 6]
- Errors: [-1, -1, -1, -1, 94]

| Metric | Value | Dominated by Outlier? |
|--------|-------|---------------------|
| MAE | 19.6 | No (contribution = 94/5 = 18.8) |
| MSE | 1770 | Yes (contribution = 8836/5 = 1767) |

### 2. Interpretable in Original Units

MAE is in the same units as the target variableâ€”just like RMSE but without the square root.

**Example:**
- MAE = $500 for house price prediction
- Interpretation: "On average, predictions are off by $500"

### 3. Intuitive Meaning

MAE has a clear interpretation: the average absolute deviation from the predicted value.

## Disadvantages of MAE

### 1. Not Differentiable at Zero

The absolute value function has a "kink" at zero where the derivative is undefined:

$$\frac{d}{dx}|x| = \begin{cases} -1 & x < 0 \\ \text{undefined} & x = 0 \\ 1 & x > 0 \end{cases}$$

**Impact:** Gradient descent may have convergence issues near zero. However, subgradient methods can handle this.

### 2. May Not Penalize Large Errors Enough

Since errors are not squared, MAE doesn't strongly incentivize avoiding large mistakes:

- Error of 10 â†’ penalty of 10
- Error of 100 â†’ penalty of 100 (only 10x, not 100x like MSE)

This can be problematic when large errors are particularly costly.

### 3. May Lead to Multiple Solutions

For certain problems, MAE can have multiple optimal solutions (non-unique solutions), especially in median-based predictions.

## MAE vs MSE: When to Use Which?

| Scenario | Recommended Metric |
|----------|-------------------|
| **Outliers in data** | MAE |
| **Want interpretable errors** | MAE or RMSE |
| **Large errors are costly** | MSE or RMSE |
| **Need differentiability** | MSE |
| **Equal treatment of errors** | MAE |

**Rule of Thumb:**
- Use **MAE** when outliers are genuine data points you want the model to handle reasonably
- Use **MSE** when outliers are errors/noise that should heavily penalized

## Python Implementation

### Using NumPy
```python
import numpy as np

def mean_absolute_error(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

y_actual = np.array([100, 200, 150, 80, 300])
y_predicted = np.array([110, 190, 155, 85, 295])

mae = mean_absolute_error(y_actual, y_predicted)
print(f"MAE: {mae}")  # 7.0
```

### Using Scikit-Learn
```python
from sklearn.metrics import mean_absolute_error

y_actual = [100, 200, 150, 80, 300]
y_predicted = [110, 190, 155, 85, 295]

mae = mean_absolute_error(y_actual, y_predicted)
print(f"MAE: {mae}")  # 7.0
```

### In Linear Regression
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"Test MAE: {mae}")
```

## ðŸ“– From Class Notes

### The "Sharp Corner" Problem

From the class notes: MAE has a critical disadvantageâ€”it's not differentiable at the point where the error is exactly zero. MAE follows a "V" shape: y = |x|. It has a sharp, pointed corner at the origin (0,0). 

In calculus, to find a derivative (slope), the curve must be "smooth." At that sharp point of the "V," the direction of the line changes instantly from -1 to +1. Because there is no single, clear slope at that exact point, we say it is not differentiable there.

### Analogy from Lecture

The slope is just as steep at the bottom as it at the top. The optimization will likely "jump" back and forth across the center (oscillate) because it never receives a signal to slow down. This is why MAE can cause oscillation during gradient descent.

### Advantage: Robustness

The class notes emphasize: We use MAE because it is less sensitive to outliers. Even though the 'sharp corner' makes the math harder for the computer to find the exact minimum, the resulting model is often more realistic because it doesn't let one 'crazy' data point ruin the predictions for all the 'normal' ones.

---

## Interview Questions

**Q: Why is MAE more robust to outliers than MSE?**
MSE squares each error, which amplifies large errors exponentially. A single outlier with error 100 contributes 10,000 to MSE but only 100 to MAE. MAE treats all errors proportionally, so outliers don't dominate the metric.

**Q: When would you choose MAE over RMSE?**
Use MAE when you want simpler interpretation (average error in original units without the distortion of square root), or when you want all errors weighted equally regardless of magnitude. RMSE is preferred when you want to emphasize reducing large errors.

**Q: Can MAE be used with gradient descent?**
Yes, but with caveats. The absolute value function isn't differentiable at zero, so we use subgradient methods. Alternatively, Huber loss (which behaves like MSE for small errors and MAE for large errors) provides differentiability while maintaining robustness.

## Summary

Mean Absolute Error (MAE) is a robust, interpretable metric that measures average prediction error in original units. Key points:
- Treats all errors equally, robust to outliers
- Interpretable: "predictions are off by X on average"
- Not differentiable at zero (use with care in gradient descent)
- Choose MAE when outliers should not dominate; choose MSE/RMSE when large errors must be minimized

## Related Notes

- [[02. Mean Squared Error]]
- [[03. Root Mean Squared Error]]
- [[01. Loss Functions Concept]]

## Resources

- Lectures: 10-12
