---
tags: [topic]
lecture: [10]
created: 2026-02-13
---

# Loss Functions Concept

## Overview

In machine learning, a **loss function** (also called cost function or objective function) is the measure of how wrong our model's predictions are. It quantifies the "penalty" for making incorrect predictions. The entire goal of training a machine learning model is to minimize this loss function—to find the parameters that make predictions as close to actual values as possible.

Understanding loss functions is fundamental because they define what "good" means for your model. Different problems require different loss functions, and choosing the wrong one can lead to poor model performance or biased predictions.

## Loss Function vs Cost Function

While often used interchangeably, there's a subtle distinction:

- **Loss function**: The error for a **single training example**
  $$L(y, \hat{y}) = (y - \hat{y})^2$$

- **Cost function**: The **average** loss across all training examples
  $$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}L(y_i, \hat{y}_i)$$

Where $m$ is the number of training examples.

Think of it this way:
- Loss = your score on one question
- Cost (Cost function) = your average score across all questions

## Why Do We Need a Loss Function?

Without a way to measure error, we have no signal to guide learning. The loss function provides:

1. **Optimization Target**: Something for the algorithm to minimize
2. **Performance Metric**: A way to evaluate model quality
3. **Gradient Direction**: Tells us which direction to adjust parameters

## The Optimization Process

```
Training Data → Model → Predictions → Loss Calculation → Parameter Update → Repeat
```

The algorithm iteratively adjusts parameters to reduce loss until it converges to a minimum.

## Types of Loss Functions

### For Regression (Continuous Targets)

| Loss Function | Formula | Use Case |
|---------------|---------|----------|
| **MSE** | $\frac{1}{n}\sum(y - \hat{y})^2$ | Standard, penalizes large errors |
| **MAE** | $\frac{1}{n}\sum\|y - \hat{y}\|$ | Robust to outliers |
| **Huber** | Combination of MSE/MAE | Balances robustness and sensitivity |

### For Classification (Discrete Targets)

| Loss Function | Formula | Use Case |
|---------------|---------|----------|
| **Binary Cross-Entropy** | $-[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$ | Binary classification |
| **Categorical Cross-Entropy** | $-\sum y_i \log(\hat{y}_i)$ | Multi-class classification |

## Properties of Good Loss Functions

1. **Differentiable**: Allows gradient-based optimization
2. **Convex** (ideally): Ensures global minimum is findable
3. **Penalizes errors appropriately**: Large errors should be penalized more (or less, depending on use case)
4. **Interpretable**: Makes model evaluation intuitive

## Choosing the Right Loss Function

Consider these factors:

| Factor | Question | Recommendation |
|--------|----------|----------------|
| **Outliers** | Are there outliers in data? | Use MAE or Huber if yes |
| **Scale** | Is scale of target important? | Use MAE for interpretability |
| **Task** | Regression or Classification? | Use appropriate loss type |
| **Asymmetry** | Are over/under predictions equally bad? | Consider custom loss |

## Common Misconceptions

**Misconception**: "Loss function and cost function are the same thing."

**Clarification**: Loss is per-sample, cost is the average (or sum) across the dataset. In practice, people often use "loss" and "cost" interchangeably when referring to the function being optimized.

## Visual Intuition

```
        Loss
         │
    High │    ╱⎺⎺⎺⎺⎺⎺╲
         │  ╱          ╲
         │ ╱              ╲
         │╱                 ╲
         └───────────────────→
              Parameters →
              
    The model tries to find the parameters
    at the bottom of this "valley" (minimum)
```

## Interview Questions

**Q: Why do we use squared error instead of absolute error in linear regression?**
Squared error is differentiable everywhere (including at zero), which is required for gradient descent. It also penalizes large errors more heavily, encouraging the model to avoid big mistakes. However, absolute error (MAE) is more robust to outliers.

**Q: Can a model have multiple loss functions?**
Yes. Multi-task learning uses multiple loss functions, one per task. The total loss is typically a weighted sum of individual losses. Additionally, some algorithms combine loss functions (like Huber loss, which is MSE for small errors and MAE for large ones).

## Summary

Loss functions are the heart of machine learning optimization. They quantify prediction error and guide the learning process. Key takeaways:
- Loss = per-sample error, cost = average error across dataset
- Different problems require different loss functions
- MSE is standard for regression, cross-entropy for classification
- The choice of loss affects model behavior significantly

## Related Notes

- [[02. Mean Squared Error]]
- [[03. Root Mean Squared Error]]
- [[04. Mean Absolute Error]]
- [[05. Square Function]]

## Resources

- Lectures: 10-12
