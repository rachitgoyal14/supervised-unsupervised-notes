---
tags: [topic]
lecture: [10]
created: 2026-02-13
---

# Square Function

## Overview

The square function—the operation of squaring errors $(y - \hat{y})^2$—is the foundation of one of the most widely used loss functions in machine learning: Mean Squared Error (MSE). Understanding why we square errors reveals deep insights about optimization, gradient descent, and model behavior.

While seemingly simple ($x^2$), the square operation has profound mathematical properties that make it ideal for regression problems.

## Why Square the Errors?

### Problem 1: Cancellation

If we just summed raw errors:
$$\sum(y_i - \hat{y}_i)$$

Positive and negative errors would cancel:
- Error = +10 at point A
- Error = -10 at point B
- Sum = 0 (appears perfect, but predictions are wrong!)

### Solution: Absolute Value or Square

Both $|y - \hat{y}|$ and $(y - \hat{y})^2$ ensure all errors contribute positively:
- |+10| + |-10| = 10 + 10 = 20
- (+10)² + (-10)² = 100 + 100 = 200

### Why Square Over Absolute Value?

The square function offers unique advantages:

| Property | Square $(x^2)$ | Absolute $(|x|)$ |
|----------|---------------|------------------|
| **Differentiable** | Yes, everywhere | No (kink at 0) |
| **Convex** | Yes | Yes |
| **Penalizes large errors** | Heavily | Proportionally |
| **Gradient magnitude** | Grows with error | Constant |

## Key Properties of Square Function

### 1. Differentiability

The square function is smooth and differentiable everywhere:

$$\frac{d}{dx}x^2 = 2x$$

This is crucial for gradient descent—we can compute exact gradients at any point.

**Contrast with absolute value:**
$$\frac{d}{dx}|x| = \begin{cases} -1 & x < 0 \\ \text{undefined} & x = 0 \\ 1 & x > 0 \end{cases}$$

The undefined derivative at zero can cause optimization issues.

### 2. Convexity

The square function is convex (curves upward), meaning it has a single global minimum:

```
    f(x) = x²
    
         │
      9  │      ●
         │    ●
      4  │  ●
         │●
      0  │●
         └────────────
         
    Only one minimum → guaranteed convergence
```

This guarantees that gradient descent will find the global minimum for linear regression.

### 3. Penalizing Large Errors

The square function **amplifies** large errors exponentially:

| Error | Squared Error | Amplification |
|-------|--------------|---------------|
| 1 | 1 | 1x |
| 5 | 25 | 5x |
| 10 | 100 | 10x |

This strong penalization encourages models to avoid large mistakes.

**Real-world implication:**
- A prediction off by 10 units is 100x worse than being off by 1 unit
- Model prioritizes eliminating large errors
- Good when large errors are costly (e.g., medical predictions)

### 4. Growing Gradient

The gradient of square function grows with error magnitude:

$$\frac{d}{dx}x^2 = 2x$$

- At x = 1: gradient = 2 (small correction)
- At x = 10: gradient = 20 (large correction)
- At x = 100: gradient = 200 (very large correction)

This is adaptive: the model naturally takes larger steps when far from optimal and smaller steps when close.

## Mathematical Conveniences

### Closed-Form Solution

Squared error enables the Normal Equation (closed-form solution):

$$\beta = (X^TX)^{-1}X^Ty$$

This works because squared error derivatives are linear in parameters.

### Normal Distribution Connection

Under Gaussian (normal) noise assumptions, minimizing squared error is equivalent to maximum likelihood estimation. This provides theoretical justification: if errors are normally distributed, MSE is the optimal loss function.

### Second Derivative

The second derivative is constant (2), indicating uniform curvature:

$$\frac{d^2}{dx^2}x^2 = 2$$

This means:
- Gradient descent behaves predictably
- No changing curvature to complicate optimization
- Stable, consistent convergence

## Comparison: Square vs Other Functions

### vs Absolute Value (MAE)

| Aspect | Square | Absolute |
|--------|--------|----------|
| Large errors | 100x penalty | 10x penalty |
| Differentiable | Yes | No |
| Gradient | Linear growth | Constant |
| Outlier sensitivity | High | Low |
| Theoretical basis | Gaussian noise | Laplace distribution |

### vs Huber Loss

Huber combines both—behaves like square for small errors, absolute for large:

$$L_\delta(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & |y - \hat{y}| \leq \delta \\ \delta|y - \hat{y}| - \frac{1}{2}\delta^2 & |y - \hat{y}| > \delta \end{cases}$$

## Interview Questions

**Q: Why do we square errors instead of using absolute value?**
The square function is differentiable everywhere, enabling gradient-based optimization. It also penalizes large errors more heavily, which is desirable when large mistakes are particularly costly. Additionally, the squared error has nice theoretical properties—it corresponds to maximum likelihood estimation under Gaussian noise assumptions.

**Q: What are the downsides of squaring errors?**
The main downside is sensitivity to outliers. A single large error can dominate the total loss, causing the model to overfit to outliers. For datasets with many outliers, Mean Absolute Error (MAE) or Huber loss may be more appropriate.

**Q: How does the square function affect gradient descent?**
The gradient of squared error ($2 \times error$) is proportional to the error magnitude. This means:
- Large errors → large gradients → big parameter updates
- Small errors → small gradients → fine-tuned updates
This adaptive step size helps convergence, but can also cause oscillation if learning rate is too high.

## Python Example: Gradient of Squared Error

```python
import numpy as np

def squared_error_gradient(y_true, y_pred, x):
    """Gradient of MSE with respect to weights"""
    error = y_pred - y_true
    gradient = 2 * error * x
    return gradient

# Example
y_true = 100
y_pred = 80
x = 5

error = y_pred - y_true  # -20
gradient = 2 * error * x  # -200

print(f"Error: {error}")
print(f"Gradient: {gradient}")  # Negative = increase prediction
```

## Summary

The square function is fundamental to regression for good reasons:
- **Prevents error cancellation**: All errors count positively
- **Enables optimization**: Differentiable everywhere
- **Convex**: Single global minimum guaranteed
- **Adaptive gradients**: Larger errors → larger corrections
- **Theoretical basis**: Connects to maximum likelihood estimation

The heavy penalization of outliers is both a strength (when large errors matter) and weakness (when outliers are noise).

## Related Notes

- [[02. Mean Squared Error]]
- [[04. Mean Absolute Error]]
- [[01. Loss Functions Concept]]

## Resources

- Lectures: 10-12
