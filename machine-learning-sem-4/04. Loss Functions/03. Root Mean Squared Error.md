## Definition

Root Mean Squared Error (RMSE) is the square root of Mean Squared Error (MSE). It represents the **standard deviation of prediction errors** (residuals) and is expressed in the **same units as the target variable**.

## Mathematical Formula
```
RMSE = ‚àöMSE = ‚àö[(1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢)¬≤]
```

Where:
- **n** = number of data points
- **y·µ¢** = actual value for the i·µó ∞ observation
- **≈∑·µ¢** = predicted value for the i·µó ∞ observation
- **(y·µ¢ - ≈∑·µ¢)** = error/residual

---

## Relationship with MSE

RMSE is simply the square root of MSE:
```
MSE ‚Üí Square the errors ‚Üí Calculate mean
RMSE ‚Üí Square the errors ‚Üí Calculate mean ‚Üí Take square root
```

**Key Difference:**
- **MSE** is in **squared units** (e.g., dollars¬≤, meters¬≤)
- **RMSE** is in **original units** (e.g., dollars, meters)

---

## Why Take the Square Root?

### The Unit Problem with MSE

**Example: House Price Prediction**

If we're predicting house prices in dollars:
- **MSE** = 100,000,000 dollars¬≤ (squared units)
- Hard to interpret: "What does 100 million dollars¬≤ mean?"

**Solution:**
```
RMSE = ‚àö100,000,000 = $10,000
```

Now we can say: "On average, our predictions are off by about $10,000"

**Much more interpretable!**

---

## Calculation Steps

### Step-by-Step Process

1. **Calculate errors** for each prediction:
```
   error = y·µ¢ - ≈∑·µ¢
```

2. **Square each error**:
```
   squared_error = (y·µ¢ - ≈∑·µ¢)¬≤
```

3. **Calculate mean of squared errors**:
```
   MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

4. **Take the square root**:
```
   RMSE = ‚àöMSE
```

---

## Example Calculation

### Dataset

| Actual (y) | Predicted (≈∑) | Error (y - ≈∑) | Squared Error |
|------------|---------------|---------------|---------------|
| 100        | 95            | 5             | 25            |
| 150        | 155           | -5            | 25            |
| 200        | 190           | 10            | 100           |
| 180        | 185           | -5            | 25            |
| 120        | 125           | -5            | 25            |

**Step 1: Calculate MSE**
```
Sum of Squared Errors = 25 + 25 + 100 + 25 + 25 = 200
MSE = 200 / 5 = 40
```

**Step 2: Calculate RMSE**
```
RMSE = ‚àö40 ‚âà 6.32
```

**Interpretation:** On average, predictions deviate from actual values by approximately 6.32 units.

---

## Advantages of RMSE

### 1. Same Units as Target Variable 

**Most Important Advantage!**

RMSE is in the **same units** as your original data, making it highly interpretable.

**Examples:**

| Problem      | Target Unit    | MSE Unit         | RMSE Unit       |
| ------------ | -------------- | ---------------- | --------------- |
| House Prices | Dollars ($)    | Dollars¬≤ ($¬≤)    | Dollars ($)     |
| Temperature  | Celsius (¬∞C)   | Celsius¬≤ (¬∞C¬≤)   | Celsius (¬∞C)    |
| Distance     | Meters (m)     | Meters¬≤ (m¬≤)     | Meters (m)      |
| Weight       | Kilograms (kg) | Kilograms¬≤ (kg¬≤) | Kilograms (kg)  |

**Benefit:**
- Easy to communicate to non-technical stakeholders
- Can directly compare RMSE with original data values
- Intuitive understanding of model performance

---

### 2. Represents Standard Deviation of Errors

RMSE can be interpreted as the **standard deviation of residuals** (prediction errors).

**Statistical Interpretation:**

If errors are normally distributed:
- **~68%** of predictions fall within ¬±1 RMSE of actual values
- **~95%** of predictions fall within ¬±2 RMSE of actual values
- **~99.7%** of predictions fall within ¬±3 RMSE of actual values

**Example:**
```
If RMSE = $10,000 for house price prediction:
- 68% of predictions are within ¬±$10,000 of actual price
- 95% of predictions are within ¬±$20,000 of actual price
```

This gives a probabilistic understanding of model accuracy.

---

### 3. Penalizes Large Errors (Like MSE)

Because RMSE is derived from MSE, it **still penalizes large errors more heavily** than small ones.

**Example:**

| Error | Squared | Contribution to MSE | Contribution to RMSE |
|-------|---------|---------------------|----------------------|
| 1     | 1       | Low                 | Low                  |
| 5     | 25      | Moderate            | Moderate             |
| 10    | 100     | High                | High                 |

Large errors have outsized impact due to the squaring step.

---

### 4. Differentiable and Smooth

Like MSE, RMSE is **differentiable everywhere** (including at zero) and produces a smooth curve.

**Benefit:**
- Works well with gradient-based optimization
- No sharp corners that could confuse optimization algorithms

**Note:** While RMSE itself is differentiable, in practice we often optimize MSE during training (since ‚àöMSE has the same minimum as MSE) and report RMSE for interpretability.

---

### 5. Widely Used and Understood

RMSE is one of the **most popular metrics** in machine learning and statistics.

**Benefits:**
- Easy to compare results across papers and projects
- Well-established baselines for many domains
- Supported by all major ML libraries

---

### 6. Good for Comparing Models on Same Dataset

RMSE allows fair comparison between different models predicting the same target variable.

**Example: House Price Prediction**

| Model             | RMSE           |
| ----------------- | -------------- |
| Linear Regression | $25,000        |
| Random Forest     | $18,000 (Best) |
| Neural Network    | $22,000        |

Lower RMSE = Better model performance

---

## Disadvantages of RMSE

### 1. Highly Sensitive to Outliers 

**Biggest Disadvantage!**

Because RMSE involves squaring errors (like MSE), **outliers have a massive impact**.

**Example: Student Test Scores**

**Normal dataset:**
```
Actual:    [85, 90, 88, 92, 87]
Predicted: [83, 89, 90, 91, 85]
Errors:    [2, 1, -2, 1, 2]

MSE = (4 + 1 + 4 + 1 + 4) / 5 = 2.8
RMSE = ‚àö2.8 ‚âà 1.67
```

**Dataset with one outlier:**
```
Actual:    [85, 90, 88, 92, 0]  ‚Üê Outlier (student absent)
Predicted: [83, 89, 90, 91, 85]
Errors:    [2, 1, -2, 1, -85]

MSE = (4 + 1 + 4 + 1 + 7225) / 5 = 1447
RMSE = ‚àö1447 ‚âà 38.04
```

RMSE increased from **1.67 to 38.04** (23x increase!) due to a single outlier.

**Problem:**
- One bad prediction can make the entire model look terrible
- Not robust to noisy or corrupted data
- May overemphasize rare extreme cases

---

### 2. Scale Dependent

RMSE values depend on the **scale of the target variable**, making comparisons across different datasets difficult.

**Example:**

| Problem | RMSE | Good or Bad? |
|---------|------|--------------|
| House Prices ($) | 50,000 | ? |
| Temperature (¬∞C) | 2.5 | ? |
| Stock Returns (%) | 0.5 | ? |

**Problem:** Can't directly compare these RMSE values because they're in different units and scales.

**Solution:** Use scale-independent metrics like:
- **MAPE** (Mean Absolute Percentage Error)
- **Normalized RMSE**
- **R¬≤ Score**

---

### 3. Not Robust to Different Data Ranges

Models predicting large values naturally have larger RMSE than models predicting small values.

**Example:**

| Model | Target Range | RMSE | Performance |
|-------|--------------|------|-------------|
| Model A | $10,000 - $50,000 | 5,000 | Good? |
| Model B | $500,000 - $2,000,000 | 50,000 | Good? |

Both might be equally accurate relative to their scale, but Model B has 10x higher RMSE!

---

### 4. Asymmetric Treatment of Over/Under Predictions

RMSE treats **overestimation** and **underestimation** equally.

**Example in Real Estate:**
```
Scenario 1: Predict $500k, Actual $400k ‚Üí Error = $100k ‚Üí Squared = $10 billion
Scenario 2: Predict $300k, Actual $400k ‚Üí Error = -$100k ‚Üí Squared = $10 billion
```

Both contribute equally to RMSE, but:
- **Overestimating** might cause buyer to lose interest
- **Underestimating** might cause seller to lose money

The business impact is different, but RMSE treats them the same.

---

### 5. Square Root Makes Gradient Descent Slightly Harder

During optimization, the square root in RMSE adds computational complexity.

**Derivative of RMSE:**
```
‚àÇ(RMSE)/‚àÇ≈∑ = ‚àÇ(‚àöMSE)/‚àÇ≈∑ = (1/2‚àöMSE) ¬∑ ‚àÇ(MSE)/‚àÇ≈∑
```

**In Practice:**
- Most libraries optimize MSE directly during training
- Report RMSE for interpretability after training
- Minimizing MSE is equivalent to minimizing RMSE (same optimal parameters)

---

### 6. Can Be Difficult to Set Target Values

What is a "good" RMSE value?

**Problem:** No universal threshold. It depends on:
- Domain and problem context
- Scale of target variable
- Business requirements
- Noise in data

**Example:**
- RMSE = 5¬∞C for weather prediction might be acceptable
- RMSE = 5¬∞C for medical thermometer would be terrible!

---

## RMSE vs MSE: When to Use Which?

| Aspect                      | MSE                  | RMSE                      |
| --------------------------- | -------------------- | ------------------------- |
| **Units**                   | Squared units        | Original units            |
| **Interpretability**        | Harder               | Easier                    |
| **Optimization**            | Direct optimization  | Optimize MSE, report RMSE |
| **Sensitivity to outliers** | Very high            | Very high                 |
| **Mathematical properties** | Simpler              | Slightly more complex     |
| **Reporting results**       | Less common          | More common               |

**General Rule:**
- **Optimize MSE** during training (simpler math)
- **Report RMSE** when presenting results (better interpretability)

---

## Comparison with Other Metrics

### RMSE vs MAE (Mean Absolute Error)

| Metric   | Formula         | Outlier Sensitivity | Units          | Use Case                               |
| -------- | --------------- | ------------------- | -------------- | -------------------------------------- |
| **RMSE** | ‚àö[(1/n)Œ£(y-≈∑)¬≤] | **High**            | Original units | When large errors are particularly bad |
| **MAE**  | (1/n)Œ£\|y-≈∑\|   | **Low**             | Original units | When outliers should not dominate      |

**Example:**
Errors: [2, 3, 2, 3, 100] ‚Üê One outlier

$$ MAE = (2 + 32 + 3 + 100) / 5 = 22 RMSE = ‚àö[(4 + 9 + 4 + 9 + 10000) / 5] = ‚àö2005.2 ‚âà 44.78 $$

MAE is less affected by the outlier!

**Benefit:** Now you can compare models across different problems!

---

## Key Takeaways

1. **RMSE = ‚àöMSE** - Simply the square root of Mean Squared Error
2. **Same units as target** - Makes it highly interpretable 
3. **Represents standard deviation** of prediction errors
4. **Highly sensitive to outliers** - Large errors dominate the metric 
5. **Scale-dependent** - Hard to compare across different problems
6. **Widely used** - Industry standard for reporting regression performance
7. **Use for interpretation** - Report RMSE to stakeholders
8. **Optimize MSE instead** - During training, optimize MSE (simpler math)

---

## üìñ From Class Notes

### Why RMSE is in Original Units

From the class notes: RMSE has the error available in the same units as the input, making it easy to understand for humans. For example, if predicting house prices in rupees, RMSE gives the error in rupees rather than rupees¬≤.

### The Derivative Challenge

An important insight from lecture: finding the gradient for RMSE is "messy." If U is the mean squared error, then RMSE = ‚àöU. To find the derivative with respect to predictions, we use the chain rule:

$$\frac{\partial}{\partial \hat{y}}\sqrt{U} = \frac{1}{2\sqrt{U}} \cdot \frac{\partial U}{\partial \hat{y}}$$

Notice that RMSE is in the denominator‚Äîas the model becomes perfect and error approaches zero, you're essentially dividing by zero. The slope becomes undefined or infinite at the exact moment the model reaches the correct answer. This is why we optimize MSE during training but report RMSE for interpretability.

### Practical Tip

> "Use MSE to train the model (because the math is 'clean' and the slope is 'smooth') but report RMSE to stakeholders (because the units are 'intuitive')."

---

## Further Reading

- **"Hands-On Machine Learning" by Aur√©lien G√©ron** - Chapter 2: End-to-End Machine Learning Project
- **"Introduction to Statistical Learning"** - Chapter 3: Linear Regression, Section 3.1.3
- **"Deep Learning" by Goodfellow et al.** - Chapter 5: Machine Learning Basics
- **Scikit-Learn Documentation:** https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics